{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9647eb38-a83e-406b-9819-24476168ffce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 30 11:10:37 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti     Off |   00000000:03:00.0 Off |                  N/A |\n",
      "|  0%   34C    P8             13W /  250W |     380MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1405      G   /usr/lib/xorg/Xorg                            300MiB |\n",
      "|    0   N/A  N/A      1660      G   /usr/bin/gnome-shell                            7MiB |\n",
      "|    0   N/A  N/A      3461      G   /usr/lib/xorg/Xorg                             67MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d260ec4a-cb8f-440a-8cc6-760127bcc99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q autotrain-advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74730da3-2f10-4956-83ee-8677df78958c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:10:56\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mInstalling latest xformers\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:10:57\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSuccessfully installed latest xformers\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:10:57\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mInstalling latest PyTorch\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-04-30 11:10:59\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mSuccessfully installed latest PyTorch\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!autotrain setup --update-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d63304-b602-4acd-8a1f-e2b2ee69d528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (0.16.6)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from wandb) (2.0.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from wandb) (4.23.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseoma\u001b[0m (\u001b[33mseo-ma\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install wandb\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1cc30ec-2ecd-4e89-a1b5-383e1917da5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: autotrain <command> [<args>] llm [-h] [--train] [--deploy]\n",
      "                                        [--inference] [--username USERNAME]\n",
      "                                        [--backend {local-cli,spaces-a10gl,spaces-a10gs,spaces-a100,spaces-t4m,spaces-t4s,spaces-cpu,spaces-cpuf}]\n",
      "                                        [--token TOKEN] [--push-to-hub]\n",
      "                                        --model MODEL --project-name\n",
      "                                        PROJECT_NAME [--data-path DATA_PATH]\n",
      "                                        [--train-split TRAIN_SPLIT]\n",
      "                                        [--valid-split VALID_SPLIT]\n",
      "                                        [--batch-size BATCH_SIZE]\n",
      "                                        [--seed SEED] [--epochs EPOCHS]\n",
      "                                        [--gradient-accumulation GRADIENT_ACCUMULATION]\n",
      "                                        [--disable-gradient-checkpointing]\n",
      "                                        [--lr LR]\n",
      "                                        [--log {none,wandb,tensorboard}]\n",
      "                                        [--text_column TEXT_COLUMN]\n",
      "                                        [--rejected_text_column REJECTED_TEXT_COLUMN]\n",
      "                                        [--prompt-text-column PROMPT_TEXT_COLUMN]\n",
      "                                        [--model-ref MODEL_REF]\n",
      "                                        [--warmup_ratio WARMUP_RATIO]\n",
      "                                        [--optimizer OPTIMIZER]\n",
      "                                        [--scheduler SCHEDULER]\n",
      "                                        [--weight_decay WEIGHT_DECAY]\n",
      "                                        [--max_grad_norm MAX_GRAD_NORM]\n",
      "                                        [--add_eos_token]\n",
      "                                        [--block_size BLOCK_SIZE] [--peft]\n",
      "                                        [--lora_r LORA_R]\n",
      "                                        [--lora_alpha LORA_ALPHA]\n",
      "                                        [--lora_dropout LORA_DROPOUT]\n",
      "                                        [--logging_steps LOGGING_STEPS]\n",
      "                                        [--evaluation_strategy {epoch,steps,no}]\n",
      "                                        [--save_total_limit SAVE_TOTAL_LIMIT]\n",
      "                                        [--auto_find_batch_size]\n",
      "                                        [--mixed_precision {fp16,bf16,None}]\n",
      "                                        [--quantization {int4,int8,None}]\n",
      "                                        [--model_max_length MODEL_MAX_LENGTH]\n",
      "                                        [--max_prompt_length MAX_PROMPT_LENGTH]\n",
      "                                        [--max_completion_length MAX_COMPLETION_LENGTH]\n",
      "                                        [--trainer {default,dpo,sft,orpo,reward}]\n",
      "                                        [--target_modules TARGET_MODULES]\n",
      "                                        [--merge_adapter]\n",
      "                                        [--use_flash_attention_2]\n",
      "                                        [--dpo-beta DPO_BETA]\n",
      "                                        [--chat_template {tokenizer,chatml,zephyr,None}]\n",
      "                                        [--padding {left,right,None}]\n",
      "\n",
      "âœ¨ Run AutoTrain LLM\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --train               Command to train the model\n",
      "  --deploy              Command to deploy the model (limited availability)\n",
      "  --inference           Command to run inference (limited availability)\n",
      "  --username USERNAME   Hugging Face Hub Username\n",
      "  --backend {local-cli,spaces-a10gl,spaces-a10gs,spaces-a100,spaces-t4m,spaces-t4s,spaces-cpu,spaces-cpuf}\n",
      "                        Backend to use: default or spaces. Spaces backend\n",
      "                        requires push_to_hub & username. Advanced users only.\n",
      "  --token TOKEN         Your Hugging Face API token. Token must have write\n",
      "                        access to the model hub.\n",
      "  --push-to-hub         Push to hub after training will push the trained model\n",
      "                        to the Hugging Face model hub.\n",
      "  --model MODEL         Base model to use for training\n",
      "  --project-name PROJECT_NAME\n",
      "                        Output directory / repo id for trained model (must be\n",
      "                        unique on hub)\n",
      "  --data-path DATA_PATH\n",
      "                        Train dataset to use. When using cli, this should be a\n",
      "                        directory path containing training and validation data\n",
      "                        in appropriate formats\n",
      "  --train-split TRAIN_SPLIT\n",
      "                        Train dataset split to use\n",
      "  --valid-split VALID_SPLIT\n",
      "                        Validation dataset split to use\n",
      "  --batch-size BATCH_SIZE, --train-batch-size BATCH_SIZE\n",
      "                        Training batch size to use\n",
      "  --seed SEED           Random seed for reproducibility\n",
      "  --epochs EPOCHS       Number of training epochs\n",
      "  --gradient-accumulation GRADIENT_ACCUMULATION, --gradient-accumulation GRADIENT_ACCUMULATION\n",
      "                        Gradient accumulation steps\n",
      "  --disable-gradient-checkpointing, --disable-gradient-checkpointing, --disable-gc\n",
      "                        Disable gradient checkpointing\n",
      "  --lr LR               Learning rate\n",
      "  --log {none,wandb,tensorboard}\n",
      "                        Use experiment tracking\n",
      "  --text_column TEXT_COLUMN, --text-column TEXT_COLUMN\n",
      "                        Specify the dataset column to use for text data. This\n",
      "                        parameter is essential for models processing textual\n",
      "                        information. Default is 'text'.\n",
      "  --rejected_text_column REJECTED_TEXT_COLUMN, --rejected-text-column REJECTED_TEXT_COLUMN\n",
      "                        Define the column to use for storing rejected text\n",
      "                        entries, which are typically entries that do not meet\n",
      "                        certain criteria for processing. Default is\n",
      "                        'rejected'. Used only for orpo, dpo and reward\n",
      "                        trainerss\n",
      "  --prompt-text-column PROMPT_TEXT_COLUMN, --prompt-text-column PROMPT_TEXT_COLUMN\n",
      "                        Identify the column that contains prompt text for\n",
      "                        tasks requiring contextual inputs, such as\n",
      "                        conversation or completion generation. Default is\n",
      "                        'prompt'. Used only for dpo trainer\n",
      "  --model-ref MODEL_REF\n",
      "                        Reference model to use for DPO when not using PEFT\n",
      "  --warmup_ratio WARMUP_RATIO, --warmup-ratio WARMUP_RATIO\n",
      "                        Set the proportion of training allocated to warming up\n",
      "                        the learning rate, which can enhance model stability\n",
      "                        and performance at the start of training. Default is\n",
      "                        0.1\n",
      "  --optimizer OPTIMIZER\n",
      "                        Choose the optimizer algorithm for training the model.\n",
      "                        Different optimizers can affect the training speed and\n",
      "                        model performance. 'adamw_torch' is used by default.\n",
      "  --scheduler SCHEDULER\n",
      "                        Select the learning rate scheduler to adjust the\n",
      "                        learning rate based on the number of epochs. 'linear'\n",
      "                        decreases the learning rate linearly from the initial\n",
      "                        lr set. Default is 'linear'. Try 'cosine' for a cosine\n",
      "                        annealing schedule.\n",
      "  --weight_decay WEIGHT_DECAY, --weight-decay WEIGHT_DECAY\n",
      "                        Define the weight decay rate for regularization, which\n",
      "                        helps prevent overfitting by penalizing larger\n",
      "                        weights. Default is 0.0\n",
      "  --max_grad_norm MAX_GRAD_NORM, --max-grad-norm MAX_GRAD_NORM\n",
      "                        Set the maximum norm for gradient clipping, which is\n",
      "                        critical for preventing gradients from exploding\n",
      "                        during backpropagation. Default is 1.0.\n",
      "  --add_eos_token, --add-eos-token\n",
      "                        Toggle whether to automatically add an End Of Sentence\n",
      "                        (EOS) token at the end of texts, which can be critical\n",
      "                        for certain types of models like language models. Only\n",
      "                        used for `default` trainer\n",
      "  --block_size BLOCK_SIZE, --block-size BLOCK_SIZE\n",
      "                        Specify the block size for processing sequences. This\n",
      "                        is maximum sequence length or length of one block of\n",
      "                        text. Setting to -1 determines block size\n",
      "                        automatically. Default is -1.\n",
      "  --peft, --use-peft    Enable LoRA-PEFT\n",
      "  --lora_r LORA_R, --lora-r LORA_R\n",
      "                        Set the 'r' parameter for Low-Rank Adaptation (LoRA).\n",
      "                        Default is 16.\n",
      "  --lora_alpha LORA_ALPHA, --lora-alpha LORA_ALPHA\n",
      "                        Specify the 'alpha' parameter for LoRA. Default is 32.\n",
      "  --lora_dropout LORA_DROPOUT, --lora-dropout LORA_DROPOUT\n",
      "                        Set the dropout rate within the LoRA layers to help\n",
      "                        prevent overfitting during adaptation. Default is\n",
      "                        0.05.\n",
      "  --logging_steps LOGGING_STEPS, --logging-steps LOGGING_STEPS\n",
      "                        Determine how often to log training progress in terms\n",
      "                        of steps. Setting it to '-1' determines logging steps\n",
      "                        automatically.\n",
      "  --evaluation_strategy {epoch,steps,no}, --evaluation-strategy {epoch,steps,no}\n",
      "                        Choose how frequently to evaluate the model's\n",
      "                        performance, with 'epoch' as the default, meaning at\n",
      "                        the end of each training epoch\n",
      "  --save_total_limit SAVE_TOTAL_LIMIT, --save-total-limit SAVE_TOTAL_LIMIT\n",
      "                        Limit the total number of saved model checkpoints to\n",
      "                        manage disk usage effectively. Default is to save only\n",
      "                        the latest checkpoint\n",
      "  --auto_find_batch_size, --auto-find-batch-size\n",
      "                        Automatically determine the optimal batch size based\n",
      "                        on system capabilities to maximize efficiency.\n",
      "  --mixed_precision {fp16,bf16,None}, --mixed-precision {fp16,bf16,None}\n",
      "                        Choose the precision mode for training to optimize\n",
      "                        performance and memory usage. Options are 'fp16',\n",
      "                        'bf16', or None for default precision. Default is\n",
      "                        None.\n",
      "  --quantization {int4,int8,None}, --quantization {int4,int8,None}\n",
      "                        Choose the quantization level to reduce model size and\n",
      "                        potentially increase inference speed. Options include\n",
      "                        'int4', 'int8', or None. Enabling requires --peft\n",
      "  --model_max_length MODEL_MAX_LENGTH, --model-max-length MODEL_MAX_LENGTH\n",
      "                        Set the maximum length for the model to process in a\n",
      "                        single batch, which can affect both performance and\n",
      "                        memory usage. Default is 1024\n",
      "  --max_prompt_length MAX_PROMPT_LENGTH, --max-prompt-length MAX_PROMPT_LENGTH\n",
      "                        Specify the maximum length for prompts used in\n",
      "                        training, particularly relevant for tasks requiring\n",
      "                        initial contextual input. Used only for `orpo`\n",
      "                        trainer.\n",
      "  --max_completion_length MAX_COMPLETION_LENGTH, --max-completion-length MAX_COMPLETION_LENGTH\n",
      "                        Completion length to use, for orpo: encoder-decoder\n",
      "                        models only\n",
      "  --trainer {default,dpo,sft,orpo,reward}\n",
      "                        Trainer type to use\n",
      "  --target_modules TARGET_MODULES, --target-modules TARGET_MODULES\n",
      "                        Identify specific modules within the model\n",
      "                        architecture to target with adaptations or\n",
      "                        optimizations, such as LoRA. Comma separated list of\n",
      "                        module names. Default is 'all-linear'.\n",
      "  --merge_adapter, --merge-adapter\n",
      "                        Use this flag to merge PEFT adapter with the model\n",
      "  --use_flash_attention_2, --use-flash-attention-2, --use-fa2\n",
      "                        Use flash attention 2\n",
      "  --dpo-beta DPO_BETA, --dpo-beta DPO_BETA\n",
      "                        Beta for DPO trainer\n",
      "  --chat_template {tokenizer,chatml,zephyr,None}, --chat-template {tokenizer,chatml,zephyr,None}\n",
      "                        Apply a specific template for chat-based interactions,\n",
      "                        with options including 'tokenizer', 'chatml',\n",
      "                        'zephyr', or None. This setting can shape the model's\n",
      "                        conversational behavior.\n",
      "  --padding {left,right,None}, --padding {left,right,None}\n",
      "                        Specify the padding direction for sequences, critical\n",
      "                        for models sensitive to input alignment. Options\n",
      "                        include 'left', 'right', or None\n"
     ]
    }
   ],
   "source": [
    "!autotrain llm --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e49504-57d6-429a-9085-fe9c454fcaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:46:18\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m343\u001b[0m - \u001b[1mRunning LLM\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-05-02 10:46:19\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m180\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: train, inference, backend, func, deploy, version\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:46:19\u001b[0m | \u001b[36mautotrain.backend\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:46:19\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m349\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 'Llama2-KorQuAD2-dev-finetuning/training_params.json']\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:46:19\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1m{'model': 'hyunseoki/ko-ref-llama2-7b', 'project_name': 'Llama2-KorQuAD2-dev-finetuning', 'data_path': 'seoma/korquad2-dev', 'train_split': 'train', 'valid_split': None, 'add_eos_token': False, 'block_size': -1, 'model_max_length': 256, 'padding': None, 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'wandb', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'evaluation_strategy': 'epoch', 'save_total_limit': 5, 'auto_find_batch_size': False, 'mixed_precision': None, 'lr': 0.0002, 'epochs': 20, 'batch_size': 8, 'warmup_ratio': 0.1, 'gradient_accumulation': 1, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': 'int4', 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'prompt', 'text_column': 'text', 'rejected_text_column': 'rejected', 'push_to_hub': True, 'username': 'seoma', 'token': '*****'}\u001b[0m\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:46:29\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mTrain data: Dataset({\n",
      "    features: ['text', 'answer', 'question'],\n",
      "    num_rows: 10159\n",
      "})\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:46:29\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:46:29\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1mcreating training arguments...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:46:29\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m220\u001b[0m - \u001b[1mLogging steps: 25\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:46:29\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m269\u001b[0m - \u001b[1mloading model config...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:46:29\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m281\u001b[0m - \u001b[1mloading model...\u001b[0m\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.62s/it]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:46:34\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m349\u001b[0m - \u001b[1mmodel dtype: torch.float16\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:46:34\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mpreparing peft model...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:46:34\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m415\u001b[0m - \u001b[1mUsing block size 256\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:46:34\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m477\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
      "/home/r2soft/anaconda3/envs/jsk_KorQuAD/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseoma\u001b[0m (\u001b[33mseo-ma\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/r2soft/jsk/wandb/run-20240502_104635-36hrg0wi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msnowy-galaxy-5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/seo-ma/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/seo-ma/huggingface/runs/36hrg0wi\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:46:41\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_train_begin\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mStarting to train...\u001b[0m\n",
      "  0%|                                     | 25/23260 [04:33<70:55:49, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:51:14\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.6612, 'grad_norm': 0.5648978352546692, 'learning_rate': 2.1496130696474635e-06, 'epoch': 0.021496130696474634}\u001b[0m\n",
      "{'loss': 1.6612, 'grad_norm': 0.5648978352546692, 'learning_rate': 2.1496130696474635e-06, 'epoch': 0.02}\n",
      "  0%|                                     | 50/23260 [09:07<70:55:02, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 10:55:49\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.5881, 'grad_norm': 0.7776477336883545, 'learning_rate': 4.299226139294927e-06, 'epoch': 0.04299226139294927}\u001b[0m\n",
      "{'loss': 1.5881, 'grad_norm': 0.7776477336883545, 'learning_rate': 4.299226139294927e-06, 'epoch': 0.04}\n",
      "  0%|                                     | 75/23260 [13:42<70:45:17, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 11:00:23\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.5182, 'grad_norm': 0.7603458762168884, 'learning_rate': 6.448839208942391e-06, 'epoch': 0.0644883920894239}\u001b[0m\n",
      "{'loss': 1.5182, 'grad_norm': 0.7603458762168884, 'learning_rate': 6.448839208942391e-06, 'epoch': 0.06}\n",
      "  0%|â–                                   | 100/23260 [18:17<70:44:33, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 11:04:58\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.3514, 'grad_norm': 0.6039064526557922, 'learning_rate': 8.598452278589854e-06, 'epoch': 0.08598452278589853}\u001b[0m\n",
      "{'loss': 1.3514, 'grad_norm': 0.6039064526557922, 'learning_rate': 8.598452278589854e-06, 'epoch': 0.09}\n",
      "  1%|â–                                   | 125/23260 [22:52<70:34:30, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 11:09:33\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.2285, 'grad_norm': 0.595846951007843, 'learning_rate': 1.0748065348237319e-05, 'epoch': 0.10748065348237318}\u001b[0m\n",
      "{'loss': 1.2285, 'grad_norm': 0.595846951007843, 'learning_rate': 1.0748065348237319e-05, 'epoch': 0.11}\n",
      "  1%|â–                                   | 150/23260 [27:27<70:31:57, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 11:14:08\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.1649, 'grad_norm': 1.1454675197601318, 'learning_rate': 1.2897678417884782e-05, 'epoch': 0.1289767841788478}\u001b[0m\n",
      "{'loss': 1.1649, 'grad_norm': 1.1454675197601318, 'learning_rate': 1.2897678417884782e-05, 'epoch': 0.13}\n",
      "  1%|â–Ž                                   | 175/23260 [32:01<70:28:02, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 11:18:42\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.099, 'grad_norm': 0.7550520300865173, 'learning_rate': 1.5047291487532245e-05, 'epoch': 0.15047291487532244}\u001b[0m\n",
      "{'loss': 1.099, 'grad_norm': 0.7550520300865173, 'learning_rate': 1.5047291487532245e-05, 'epoch': 0.15}\n",
      "  1%|â–Ž                                   | 200/23260 [36:36<70:23:13, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 11:23:17\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.0242, 'grad_norm': 0.4619554579257965, 'learning_rate': 1.7196904557179708e-05, 'epoch': 0.17196904557179707}\u001b[0m\n",
      "{'loss': 1.0242, 'grad_norm': 0.4619554579257965, 'learning_rate': 1.7196904557179708e-05, 'epoch': 0.17}\n",
      "  1%|â–Ž                                   | 225/23260 [41:11<70:21:34, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 11:27:52\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 1.0009, 'grad_norm': 0.5629324316978455, 'learning_rate': 1.934651762682717e-05, 'epoch': 0.1934651762682717}\u001b[0m\n",
      "{'loss': 1.0009, 'grad_norm': 0.5629324316978455, 'learning_rate': 1.934651762682717e-05, 'epoch': 0.19}\n",
      "  1%|â–                                   | 250/23260 [45:46<70:14:44, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 11:32:27\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.9882, 'grad_norm': 0.49619635939598083, 'learning_rate': 2.1496130696474638e-05, 'epoch': 0.21496130696474636}\u001b[0m\n",
      "{'loss': 0.9882, 'grad_norm': 0.49619635939598083, 'learning_rate': 2.1496130696474638e-05, 'epoch': 0.21}\n",
      "  1%|â–                                   | 275/23260 [50:21<70:12:20, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 11:37:02\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.9722, 'grad_norm': 0.5869247913360596, 'learning_rate': 2.36457437661221e-05, 'epoch': 0.236457437661221}\u001b[0m\n",
      "{'loss': 0.9722, 'grad_norm': 0.5869247913360596, 'learning_rate': 2.36457437661221e-05, 'epoch': 0.24}\n",
      "  1%|â–                                   | 300/23260 [54:55<70:08:26, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 11:41:37\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.9367, 'grad_norm': 0.5666379928588867, 'learning_rate': 2.5795356835769564e-05, 'epoch': 0.2579535683576956}\u001b[0m\n",
      "{'loss': 0.9367, 'grad_norm': 0.5666379928588867, 'learning_rate': 2.5795356835769564e-05, 'epoch': 0.26}\n",
      "  1%|â–Œ                                   | 325/23260 [59:30<70:00:53, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 11:46:11\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.9185, 'grad_norm': 0.7115950584411621, 'learning_rate': 2.794496990541703e-05, 'epoch': 0.2794496990541703}\u001b[0m\n",
      "{'loss': 0.9185, 'grad_norm': 0.7115950584411621, 'learning_rate': 2.794496990541703e-05, 'epoch': 0.28}\n",
      "  2%|â–Œ                                 | 350/23260 [1:04:05<69:56:40, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 11:50:46\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.9185, 'grad_norm': 0.4975413382053375, 'learning_rate': 3.009458297506449e-05, 'epoch': 0.3009458297506449}\u001b[0m\n",
      "{'loss': 0.9185, 'grad_norm': 0.4975413382053375, 'learning_rate': 3.009458297506449e-05, 'epoch': 0.3}\n",
      "  2%|â–Œ                                 | 375/23260 [1:08:40<69:52:01, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 11:55:21\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.9098, 'grad_norm': 0.48471397161483765, 'learning_rate': 3.2244196044711957e-05, 'epoch': 0.32244196044711954}\u001b[0m\n",
      "{'loss': 0.9098, 'grad_norm': 0.48471397161483765, 'learning_rate': 3.2244196044711957e-05, 'epoch': 0.32}\n",
      "  2%|â–Œ                                 | 400/23260 [1:13:15<69:48:24, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 11:59:56\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.905, 'grad_norm': 0.5872053503990173, 'learning_rate': 3.4393809114359416e-05, 'epoch': 0.34393809114359414}\u001b[0m\n",
      "{'loss': 0.905, 'grad_norm': 0.5872053503990173, 'learning_rate': 3.4393809114359416e-05, 'epoch': 0.34}\n",
      "  2%|â–Œ                                 | 425/23260 [1:17:49<69:42:49, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 12:04:30\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.9281, 'grad_norm': 0.5260793566703796, 'learning_rate': 3.654342218400688e-05, 'epoch': 0.3654342218400688}\u001b[0m\n",
      "{'loss': 0.9281, 'grad_norm': 0.5260793566703796, 'learning_rate': 3.654342218400688e-05, 'epoch': 0.37}\n",
      "  2%|â–‹                                 | 450/23260 [1:22:24<69:38:07, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 12:09:05\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.9039, 'grad_norm': 0.5703884363174438, 'learning_rate': 3.869303525365434e-05, 'epoch': 0.3869303525365434}\u001b[0m\n",
      "{'loss': 0.9039, 'grad_norm': 0.5703884363174438, 'learning_rate': 3.869303525365434e-05, 'epoch': 0.39}\n",
      "  2%|â–‹                                 | 475/23260 [1:26:58<69:32:21, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 12:13:40\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8977, 'grad_norm': 0.7694970369338989, 'learning_rate': 4.084264832330181e-05, 'epoch': 0.40842648323301806}\u001b[0m\n",
      "{'loss': 0.8977, 'grad_norm': 0.7694970369338989, 'learning_rate': 4.084264832330181e-05, 'epoch': 0.41}\n",
      "  2%|â–‹                                 | 500/23260 [1:31:33<69:26:36, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 12:18:14\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.9248, 'grad_norm': 0.4515506327152252, 'learning_rate': 4.2992261392949275e-05, 'epoch': 0.4299226139294927}\u001b[0m\n",
      "{'loss': 0.9248, 'grad_norm': 0.4515506327152252, 'learning_rate': 4.2992261392949275e-05, 'epoch': 0.43}\n",
      "  2%|â–Š                                 | 525/23260 [1:36:08<69:24:41, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 12:22:49\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.9146, 'grad_norm': 0.49731096625328064, 'learning_rate': 4.5141874462596735e-05, 'epoch': 0.4514187446259673}\u001b[0m\n",
      "{'loss': 0.9146, 'grad_norm': 0.49731096625328064, 'learning_rate': 4.5141874462596735e-05, 'epoch': 0.45}\n",
      "  2%|â–Š                                 | 550/23260 [1:40:42<69:14:44, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 12:27:23\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.9129, 'grad_norm': 0.5377106666564941, 'learning_rate': 4.72914875322442e-05, 'epoch': 0.472914875322442}\u001b[0m\n",
      "{'loss': 0.9129, 'grad_norm': 0.5377106666564941, 'learning_rate': 4.72914875322442e-05, 'epoch': 0.47}\n",
      "  2%|â–Š                                 | 575/23260 [1:45:17<69:14:34, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 12:31:58\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8819, 'grad_norm': 0.5083344578742981, 'learning_rate': 4.944110060189166e-05, 'epoch': 0.4944110060189166}\u001b[0m\n",
      "{'loss': 0.8819, 'grad_norm': 0.5083344578742981, 'learning_rate': 4.944110060189166e-05, 'epoch': 0.49}\n",
      "  3%|â–‰                                 | 600/23260 [1:49:52<69:12:57, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 12:36:33\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8921, 'grad_norm': 0.4431523084640503, 'learning_rate': 5.159071367153913e-05, 'epoch': 0.5159071367153912}\u001b[0m\n",
      "{'loss': 0.8921, 'grad_norm': 0.4431523084640503, 'learning_rate': 5.159071367153913e-05, 'epoch': 0.52}\n",
      "  3%|â–‰                                 | 625/23260 [1:54:27<69:07:00, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 12:41:08\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.9049, 'grad_norm': 0.5125850439071655, 'learning_rate': 5.374032674118659e-05, 'epoch': 0.5374032674118658}\u001b[0m\n",
      "{'loss': 0.9049, 'grad_norm': 0.5125850439071655, 'learning_rate': 5.374032674118659e-05, 'epoch': 0.54}\n",
      "  3%|â–‰                                 | 650/23260 [1:59:01<69:01:37, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 12:45:42\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8644, 'grad_norm': 0.4546653628349304, 'learning_rate': 5.588993981083406e-05, 'epoch': 0.5588993981083406}\u001b[0m\n",
      "{'loss': 0.8644, 'grad_norm': 0.4546653628349304, 'learning_rate': 5.588993981083406e-05, 'epoch': 0.56}\n",
      "  3%|â–‰                                 | 675/23260 [2:03:36<68:53:48, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 12:50:17\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8908, 'grad_norm': 0.44953176379203796, 'learning_rate': 5.803955288048152e-05, 'epoch': 0.5803955288048152}\u001b[0m\n",
      "{'loss': 0.8908, 'grad_norm': 0.44953176379203796, 'learning_rate': 5.803955288048152e-05, 'epoch': 0.58}\n",
      "  3%|â–ˆ                                 | 700/23260 [2:08:11<68:52:00, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 12:54:52\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8566, 'grad_norm': 0.5075637698173523, 'learning_rate': 6.018916595012898e-05, 'epoch': 0.6018916595012898}\u001b[0m\n",
      "{'loss': 0.8566, 'grad_norm': 0.5075637698173523, 'learning_rate': 6.018916595012898e-05, 'epoch': 0.6}\n",
      "  3%|â–ˆ                                 | 725/23260 [2:12:46<68:48:21, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 12:59:27\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.9154, 'grad_norm': 0.5628504157066345, 'learning_rate': 6.233877901977643e-05, 'epoch': 0.6233877901977644}\u001b[0m\n",
      "{'loss': 0.9154, 'grad_norm': 0.5628504157066345, 'learning_rate': 6.233877901977643e-05, 'epoch': 0.62}\n",
      "  3%|â–ˆ                                 | 750/23260 [2:17:20<68:40:36, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 13:04:01\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.9005, 'grad_norm': 0.6006237268447876, 'learning_rate': 6.448839208942391e-05, 'epoch': 0.6448839208942391}\u001b[0m\n",
      "{'loss': 0.9005, 'grad_norm': 0.6006237268447876, 'learning_rate': 6.448839208942391e-05, 'epoch': 0.64}\n",
      "  3%|â–ˆâ–                                | 775/23260 [2:21:55<68:37:56, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 13:08:36\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8689, 'grad_norm': 0.5433245897293091, 'learning_rate': 6.663800515907137e-05, 'epoch': 0.6663800515907137}\u001b[0m\n",
      "{'loss': 0.8689, 'grad_norm': 0.5433245897293091, 'learning_rate': 6.663800515907137e-05, 'epoch': 0.67}\n",
      "  3%|â–ˆâ–                                | 800/23260 [2:26:29<68:33:07, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 13:13:11\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8657, 'grad_norm': 0.46543845534324646, 'learning_rate': 6.878761822871883e-05, 'epoch': 0.6878761822871883}\u001b[0m\n",
      "{'loss': 0.8657, 'grad_norm': 0.46543845534324646, 'learning_rate': 6.878761822871883e-05, 'epoch': 0.69}\n",
      "  4%|â–ˆâ–                                | 825/23260 [2:31:04<68:26:16, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 13:17:45\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.886, 'grad_norm': 0.6009692549705505, 'learning_rate': 7.09372312983663e-05, 'epoch': 0.709372312983663}\u001b[0m\n",
      "{'loss': 0.886, 'grad_norm': 0.6009692549705505, 'learning_rate': 7.09372312983663e-05, 'epoch': 0.71}\n",
      "  4%|â–ˆâ–                                | 850/23260 [2:35:39<68:23:05, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 13:22:20\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8485, 'grad_norm': 0.5752964615821838, 'learning_rate': 7.308684436801377e-05, 'epoch': 0.7308684436801376}\u001b[0m\n",
      "{'loss': 0.8485, 'grad_norm': 0.5752964615821838, 'learning_rate': 7.308684436801377e-05, 'epoch': 0.73}\n",
      "  4%|â–ˆâ–Ž                                | 875/23260 [2:40:13<68:16:16, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 13:26:54\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.882, 'grad_norm': 0.38567912578582764, 'learning_rate': 7.523645743766122e-05, 'epoch': 0.7523645743766122}\u001b[0m\n",
      "{'loss': 0.882, 'grad_norm': 0.38567912578582764, 'learning_rate': 7.523645743766122e-05, 'epoch': 0.75}\n",
      "  4%|â–ˆâ–Ž                                | 900/23260 [2:44:48<68:17:58, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 13:31:29\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8656, 'grad_norm': 0.4636295437812805, 'learning_rate': 7.738607050730869e-05, 'epoch': 0.7738607050730868}\u001b[0m\n",
      "{'loss': 0.8656, 'grad_norm': 0.4636295437812805, 'learning_rate': 7.738607050730869e-05, 'epoch': 0.77}\n",
      "  4%|â–ˆâ–Ž                                | 925/23260 [2:49:23<68:13:47, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 13:36:04\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.9002, 'grad_norm': 0.4116656482219696, 'learning_rate': 7.953568357695615e-05, 'epoch': 0.7953568357695615}\u001b[0m\n",
      "{'loss': 0.9002, 'grad_norm': 0.4116656482219696, 'learning_rate': 7.953568357695615e-05, 'epoch': 0.8}\n",
      "  4%|â–ˆâ–                                | 950/23260 [2:53:57<68:06:10, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 13:40:38\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8719, 'grad_norm': 0.45807355642318726, 'learning_rate': 8.168529664660362e-05, 'epoch': 0.8168529664660361}\u001b[0m\n",
      "{'loss': 0.8719, 'grad_norm': 0.45807355642318726, 'learning_rate': 8.168529664660362e-05, 'epoch': 0.82}\n",
      "  4%|â–ˆâ–                                | 975/23260 [2:58:32<68:01:25, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 13:45:13\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8933, 'grad_norm': 0.3816756010055542, 'learning_rate': 8.383490971625107e-05, 'epoch': 0.8383490971625107}\u001b[0m\n",
      "{'loss': 0.8933, 'grad_norm': 0.3816756010055542, 'learning_rate': 8.383490971625107e-05, 'epoch': 0.84}\n",
      "  4%|â–ˆâ–                               | 1000/23260 [3:03:07<67:56:13, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 13:49:48\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8585, 'grad_norm': 0.4126531183719635, 'learning_rate': 8.598452278589855e-05, 'epoch': 0.8598452278589854}\u001b[0m\n",
      "{'loss': 0.8585, 'grad_norm': 0.4126531183719635, 'learning_rate': 8.598452278589855e-05, 'epoch': 0.86}\n",
      "  4%|â–ˆâ–                               | 1025/23260 [3:07:42<67:54:38, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 13:54:23\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8853, 'grad_norm': 0.47989848256111145, 'learning_rate': 8.8134135855546e-05, 'epoch': 0.88134135855546}\u001b[0m\n",
      "{'loss': 0.8853, 'grad_norm': 0.47989848256111145, 'learning_rate': 8.8134135855546e-05, 'epoch': 0.88}\n",
      "  5%|â–ˆâ–                               | 1050/23260 [3:12:17<67:53:42, 11.01s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 13:58:58\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8767, 'grad_norm': 0.4164307415485382, 'learning_rate': 9.028374892519347e-05, 'epoch': 0.9028374892519346}\u001b[0m\n",
      "{'loss': 0.8767, 'grad_norm': 0.4164307415485382, 'learning_rate': 9.028374892519347e-05, 'epoch': 0.9}\n",
      "  5%|â–ˆâ–Œ                               | 1075/23260 [3:16:51<67:44:17, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 14:03:32\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.86, 'grad_norm': 0.41895297169685364, 'learning_rate': 9.243336199484092e-05, 'epoch': 0.9243336199484092}\u001b[0m\n",
      "{'loss': 0.86, 'grad_norm': 0.41895297169685364, 'learning_rate': 9.243336199484092e-05, 'epoch': 0.92}\n",
      "  5%|â–ˆâ–Œ                               | 1100/23260 [3:21:26<67:43:09, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 14:08:07\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8737, 'grad_norm': 0.5397502183914185, 'learning_rate': 9.45829750644884e-05, 'epoch': 0.945829750644884}\u001b[0m\n",
      "{'loss': 0.8737, 'grad_norm': 0.5397502183914185, 'learning_rate': 9.45829750644884e-05, 'epoch': 0.95}\n",
      "  5%|â–ˆâ–Œ                               | 1125/23260 [3:26:01<67:36:11, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 14:12:42\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.857, 'grad_norm': 0.4589114785194397, 'learning_rate': 9.673258813413586e-05, 'epoch': 0.9673258813413586}\u001b[0m\n",
      "{'loss': 0.857, 'grad_norm': 0.4589114785194397, 'learning_rate': 9.673258813413586e-05, 'epoch': 0.97}\n",
      "  5%|â–ˆâ–‹                               | 1150/23260 [3:30:36<67:30:37, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 14:17:17\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8646, 'grad_norm': 0.40633702278137207, 'learning_rate': 9.888220120378332e-05, 'epoch': 0.9888220120378332}\u001b[0m\n",
      "{'loss': 0.8646, 'grad_norm': 0.40633702278137207, 'learning_rate': 9.888220120378332e-05, 'epoch': 0.99}\n",
      "  5%|â–ˆâ–‹                               | 1175/23260 [3:35:11<67:24:28, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 14:21:52\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8521, 'grad_norm': 0.4219464957714081, 'learning_rate': 0.00010103181427343078, 'epoch': 1.0103181427343078}\u001b[0m\n",
      "{'loss': 0.8521, 'grad_norm': 0.4219464957714081, 'learning_rate': 0.00010103181427343078, 'epoch': 1.01}\n",
      "  5%|â–ˆâ–‹                               | 1200/23260 [3:39:45<67:23:17, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 14:26:26\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8263, 'grad_norm': 0.4163654148578644, 'learning_rate': 0.00010318142734307826, 'epoch': 1.0318142734307825}\u001b[0m\n",
      "{'loss': 0.8263, 'grad_norm': 0.4163654148578644, 'learning_rate': 0.00010318142734307826, 'epoch': 1.03}\n",
      "  5%|â–ˆâ–‹                               | 1225/23260 [3:44:20<67:15:04, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 14:31:01\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8146, 'grad_norm': 0.42629191279411316, 'learning_rate': 0.00010533104041272572, 'epoch': 1.0533104041272572}\u001b[0m\n",
      "{'loss': 0.8146, 'grad_norm': 0.42629191279411316, 'learning_rate': 0.00010533104041272572, 'epoch': 1.05}\n",
      "  5%|â–ˆâ–Š                               | 1250/23260 [3:48:55<67:09:30, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 14:35:36\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8358, 'grad_norm': 0.42116278409957886, 'learning_rate': 0.00010748065348237318, 'epoch': 1.0748065348237317}\u001b[0m\n",
      "{'loss': 0.8358, 'grad_norm': 0.42116278409957886, 'learning_rate': 0.00010748065348237318, 'epoch': 1.07}\n",
      "  5%|â–ˆâ–Š                               | 1275/23260 [3:53:30<67:07:23, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 14:40:11\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8451, 'grad_norm': 0.5321717262268066, 'learning_rate': 0.00010963026655202064, 'epoch': 1.0963026655202064}\u001b[0m\n",
      "{'loss': 0.8451, 'grad_norm': 0.5321717262268066, 'learning_rate': 0.00010963026655202064, 'epoch': 1.1}\n",
      "  6%|â–ˆâ–Š                               | 1300/23260 [3:58:04<67:02:52, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 14:44:45\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8134, 'grad_norm': 0.4072451889514923, 'learning_rate': 0.00011177987962166812, 'epoch': 1.117798796216681}\u001b[0m\n",
      "{'loss': 0.8134, 'grad_norm': 0.4072451889514923, 'learning_rate': 0.00011177987962166812, 'epoch': 1.12}\n",
      "  6%|â–ˆâ–‰                               | 1325/23260 [4:02:39<66:55:54, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 14:49:20\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.819, 'grad_norm': 0.4530957341194153, 'learning_rate': 0.00011392949269131556, 'epoch': 1.1392949269131556}\u001b[0m\n",
      "{'loss': 0.819, 'grad_norm': 0.4530957341194153, 'learning_rate': 0.00011392949269131556, 'epoch': 1.14}\n",
      "  6%|â–ˆâ–‰                               | 1350/23260 [4:07:14<66:54:20, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 14:53:55\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8246, 'grad_norm': 0.5270552039146423, 'learning_rate': 0.00011607910576096304, 'epoch': 1.1607910576096303}\u001b[0m\n",
      "{'loss': 0.8246, 'grad_norm': 0.5270552039146423, 'learning_rate': 0.00011607910576096304, 'epoch': 1.16}\n",
      "  6%|â–ˆâ–‰                               | 1375/23260 [4:11:49<66:48:02, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 14:58:30\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8179, 'grad_norm': 0.48231241106987, 'learning_rate': 0.00011822871883061048, 'epoch': 1.182287188306105}\u001b[0m\n",
      "{'loss': 0.8179, 'grad_norm': 0.48231241106987, 'learning_rate': 0.00011822871883061048, 'epoch': 1.18}\n",
      "  6%|â–ˆâ–‰                               | 1400/23260 [4:16:23<66:46:26, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 15:03:05\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8459, 'grad_norm': 0.4872475862503052, 'learning_rate': 0.00012037833190025796, 'epoch': 1.2037833190025795}\u001b[0m\n",
      "{'loss': 0.8459, 'grad_norm': 0.4872475862503052, 'learning_rate': 0.00012037833190025796, 'epoch': 1.2}\n",
      "  6%|â–ˆâ–ˆ                               | 1425/23260 [4:20:58<66:35:25, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 15:07:39\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8187, 'grad_norm': 0.4120279848575592, 'learning_rate': 0.00012252794496990543, 'epoch': 1.2252794496990542}\u001b[0m\n",
      "{'loss': 0.8187, 'grad_norm': 0.4120279848575592, 'learning_rate': 0.00012252794496990543, 'epoch': 1.23}\n",
      "  6%|â–ˆâ–ˆ                               | 1450/23260 [4:25:33<66:33:13, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 15:12:14\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8312, 'grad_norm': 0.5373961329460144, 'learning_rate': 0.00012467755803955287, 'epoch': 1.2467755803955287}\u001b[0m\n",
      "{'loss': 0.8312, 'grad_norm': 0.5373961329460144, 'learning_rate': 0.00012467755803955287, 'epoch': 1.25}\n",
      "  6%|â–ˆâ–ˆ                               | 1475/23260 [4:30:07<66:29:49, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 15:16:48\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.829, 'grad_norm': 0.4939544200897217, 'learning_rate': 0.00012682717110920036, 'epoch': 1.2682717110920034}\u001b[0m\n",
      "{'loss': 0.829, 'grad_norm': 0.4939544200897217, 'learning_rate': 0.00012682717110920036, 'epoch': 1.27}\n",
      "  6%|â–ˆâ–ˆâ–                              | 1500/23260 [4:34:42<66:30:02, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 15:21:23\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8306, 'grad_norm': 0.40462586283683777, 'learning_rate': 0.00012897678417884783, 'epoch': 1.2897678417884781}\u001b[0m\n",
      "{'loss': 0.8306, 'grad_norm': 0.40462586283683777, 'learning_rate': 0.00012897678417884783, 'epoch': 1.29}\n",
      "  7%|â–ˆâ–ˆâ–                              | 1525/23260 [4:39:17<66:19:30, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 15:25:58\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8383, 'grad_norm': 0.45006948709487915, 'learning_rate': 0.00013112639724849527, 'epoch': 1.3112639724849526}\u001b[0m\n",
      "{'loss': 0.8383, 'grad_norm': 0.45006948709487915, 'learning_rate': 0.00013112639724849527, 'epoch': 1.31}\n",
      "  7%|â–ˆâ–ˆâ–                              | 1550/23260 [4:43:51<66:13:31, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 15:30:32\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8062, 'grad_norm': 0.5505868196487427, 'learning_rate': 0.00013327601031814273, 'epoch': 1.3327601031814273}\u001b[0m\n",
      "{'loss': 0.8062, 'grad_norm': 0.5505868196487427, 'learning_rate': 0.00013327601031814273, 'epoch': 1.33}\n",
      "  7%|â–ˆâ–ˆâ–                              | 1575/23260 [4:48:26<66:12:15, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 15:35:07\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8135, 'grad_norm': 0.45036083459854126, 'learning_rate': 0.00013542562338779023, 'epoch': 1.354256233877902}\u001b[0m\n",
      "{'loss': 0.8135, 'grad_norm': 0.45036083459854126, 'learning_rate': 0.00013542562338779023, 'epoch': 1.35}\n",
      "  7%|â–ˆâ–ˆâ–Ž                              | 1600/23260 [4:53:01<66:06:18, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 15:39:42\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8266, 'grad_norm': 0.4614786207675934, 'learning_rate': 0.00013757523645743767, 'epoch': 1.3757523645743766}\u001b[0m\n",
      "{'loss': 0.8266, 'grad_norm': 0.4614786207675934, 'learning_rate': 0.00013757523645743767, 'epoch': 1.38}\n",
      "  7%|â–ˆâ–ˆâ–Ž                              | 1625/23260 [4:57:35<65:56:48, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 15:44:16\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8257, 'grad_norm': 0.5215660333633423, 'learning_rate': 0.00013972484952708513, 'epoch': 1.3972484952708513}\u001b[0m\n",
      "{'loss': 0.8257, 'grad_norm': 0.5215660333633423, 'learning_rate': 0.00013972484952708513, 'epoch': 1.4}\n",
      "  7%|â–ˆâ–ˆâ–Ž                              | 1650/23260 [5:02:10<65:56:38, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 15:48:51\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8353, 'grad_norm': 0.40136775374412537, 'learning_rate': 0.0001418744625967326, 'epoch': 1.4187446259673258}\u001b[0m\n",
      "{'loss': 0.8353, 'grad_norm': 0.40136775374412537, 'learning_rate': 0.0001418744625967326, 'epoch': 1.42}\n",
      "  7%|â–ˆâ–ˆâ–                              | 1675/23260 [5:06:45<65:54:49, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 15:53:26\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8442, 'grad_norm': 0.3665534555912018, 'learning_rate': 0.00014402407566638006, 'epoch': 1.4402407566638005}\u001b[0m\n",
      "{'loss': 0.8442, 'grad_norm': 0.3665534555912018, 'learning_rate': 0.00014402407566638006, 'epoch': 1.44}\n",
      "  7%|â–ˆâ–ˆâ–                              | 1700/23260 [5:11:19<65:49:55, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 15:58:01\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8233, 'grad_norm': 0.4080466628074646, 'learning_rate': 0.00014617368873602753, 'epoch': 1.4617368873602752}\u001b[0m\n",
      "{'loss': 0.8233, 'grad_norm': 0.4080466628074646, 'learning_rate': 0.00014617368873602753, 'epoch': 1.46}\n",
      "  7%|â–ˆâ–ˆâ–                              | 1725/23260 [5:15:54<65:45:16, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 16:02:35\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8298, 'grad_norm': 0.4026773273944855, 'learning_rate': 0.00014832330180567497, 'epoch': 1.4832330180567497}\u001b[0m\n",
      "{'loss': 0.8298, 'grad_norm': 0.4026773273944855, 'learning_rate': 0.00014832330180567497, 'epoch': 1.48}\n",
      "  8%|â–ˆâ–ˆâ–                              | 1750/23260 [5:20:29<65:37:49, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 16:07:10\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8659, 'grad_norm': 0.5542815923690796, 'learning_rate': 0.00015047291487532244, 'epoch': 1.5047291487532244}\u001b[0m\n",
      "{'loss': 0.8659, 'grad_norm': 0.5542815923690796, 'learning_rate': 0.00015047291487532244, 'epoch': 1.5}\n",
      "  8%|â–ˆâ–ˆâ–Œ                              | 1775/23260 [5:25:03<65:31:53, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 16:11:45\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8155, 'grad_norm': 0.3752422332763672, 'learning_rate': 0.00015262252794496993, 'epoch': 1.526225279449699}\u001b[0m\n",
      "{'loss': 0.8155, 'grad_norm': 0.3752422332763672, 'learning_rate': 0.00015262252794496993, 'epoch': 1.53}\n",
      "  8%|â–ˆâ–ˆâ–Œ                              | 1800/23260 [5:29:38<65:28:49, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 16:16:19\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7916, 'grad_norm': 0.4062744379043579, 'learning_rate': 0.00015477214101461737, 'epoch': 1.5477214101461736}\u001b[0m\n",
      "{'loss': 0.7916, 'grad_norm': 0.4062744379043579, 'learning_rate': 0.00015477214101461737, 'epoch': 1.55}\n",
      "  8%|â–ˆâ–ˆâ–Œ                              | 1825/23260 [5:34:13<65:22:50, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 16:20:54\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8491, 'grad_norm': 0.4717508554458618, 'learning_rate': 0.00015692175408426484, 'epoch': 1.5692175408426483}\u001b[0m\n",
      "{'loss': 0.8491, 'grad_norm': 0.4717508554458618, 'learning_rate': 0.00015692175408426484, 'epoch': 1.57}\n",
      "  8%|â–ˆâ–ˆâ–Œ                              | 1850/23260 [5:38:47<65:18:36, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 16:25:28\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8258, 'grad_norm': 0.42032256722450256, 'learning_rate': 0.0001590713671539123, 'epoch': 1.590713671539123}\u001b[0m\n",
      "{'loss': 0.8258, 'grad_norm': 0.42032256722450256, 'learning_rate': 0.0001590713671539123, 'epoch': 1.59}\n",
      "  8%|â–ˆâ–ˆâ–‹                              | 1875/23260 [5:43:22<65:14:02, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 16:30:03\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8081, 'grad_norm': 0.5895965099334717, 'learning_rate': 0.00016122098022355977, 'epoch': 1.6122098022355975}\u001b[0m\n",
      "{'loss': 0.8081, 'grad_norm': 0.5895965099334717, 'learning_rate': 0.00016122098022355977, 'epoch': 1.61}\n",
      "  8%|â–ˆâ–ˆâ–‹                              | 1900/23260 [5:47:57<65:13:49, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 16:34:38\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.824, 'grad_norm': 0.41684845089912415, 'learning_rate': 0.00016337059329320724, 'epoch': 1.6337059329320722}\u001b[0m\n",
      "{'loss': 0.824, 'grad_norm': 0.41684845089912415, 'learning_rate': 0.00016337059329320724, 'epoch': 1.63}\n",
      "  8%|â–ˆâ–ˆâ–‹                              | 1925/23260 [5:52:31<65:07:19, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 16:39:13\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8147, 'grad_norm': 0.3400021493434906, 'learning_rate': 0.0001655202063628547, 'epoch': 1.655202063628547}\u001b[0m\n",
      "{'loss': 0.8147, 'grad_norm': 0.3400021493434906, 'learning_rate': 0.0001655202063628547, 'epoch': 1.66}\n",
      "  8%|â–ˆâ–ˆâ–Š                              | 1950/23260 [5:57:06<65:03:32, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 16:43:47\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8599, 'grad_norm': 0.40749338269233704, 'learning_rate': 0.00016766981943250214, 'epoch': 1.6766981943250214}\u001b[0m\n",
      "{'loss': 0.8599, 'grad_norm': 0.40749338269233704, 'learning_rate': 0.00016766981943250214, 'epoch': 1.68}\n",
      "  8%|â–ˆâ–ˆâ–Š                              | 1975/23260 [6:01:41<64:57:20, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 16:48:22\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8502, 'grad_norm': 0.37909814715385437, 'learning_rate': 0.00016981943250214964, 'epoch': 1.6981943250214961}\u001b[0m\n",
      "{'loss': 0.8502, 'grad_norm': 0.37909814715385437, 'learning_rate': 0.00016981943250214964, 'epoch': 1.7}\n",
      "  9%|â–ˆâ–ˆâ–Š                              | 2000/23260 [6:06:16<64:54:58, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 16:52:57\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.808, 'grad_norm': 0.40516385436058044, 'learning_rate': 0.0001719690455717971, 'epoch': 1.7196904557179709}\u001b[0m\n",
      "{'loss': 0.808, 'grad_norm': 0.40516385436058044, 'learning_rate': 0.0001719690455717971, 'epoch': 1.72}\n",
      "  9%|â–ˆâ–ˆâ–Š                              | 2025/23260 [6:10:51<64:48:03, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 16:57:32\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7954, 'grad_norm': 0.4493585526943207, 'learning_rate': 0.00017411865864144454, 'epoch': 1.7411865864144453}\u001b[0m\n",
      "{'loss': 0.7954, 'grad_norm': 0.4493585526943207, 'learning_rate': 0.00017411865864144454, 'epoch': 1.74}\n",
      "  9%|â–ˆâ–ˆâ–‰                              | 2050/23260 [6:15:25<64:44:50, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 17:02:07\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.841, 'grad_norm': 0.44503623247146606, 'learning_rate': 0.000176268271711092, 'epoch': 1.76268271711092}\u001b[0m\n",
      "{'loss': 0.841, 'grad_norm': 0.44503623247146606, 'learning_rate': 0.000176268271711092, 'epoch': 1.76}\n",
      "  9%|â–ˆâ–ˆâ–‰                              | 2075/23260 [6:20:00<64:42:03, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 17:06:41\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.824, 'grad_norm': 0.4045516848564148, 'learning_rate': 0.00017841788478073947, 'epoch': 1.7841788478073948}\u001b[0m\n",
      "{'loss': 0.824, 'grad_norm': 0.4045516848564148, 'learning_rate': 0.00017841788478073947, 'epoch': 1.78}\n",
      "  9%|â–ˆâ–ˆâ–‰                              | 2100/23260 [6:24:35<64:38:35, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 17:11:16\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8289, 'grad_norm': 0.37431204319000244, 'learning_rate': 0.00018056749785038694, 'epoch': 1.8056749785038693}\u001b[0m\n",
      "{'loss': 0.8289, 'grad_norm': 0.37431204319000244, 'learning_rate': 0.00018056749785038694, 'epoch': 1.81}\n",
      "  9%|â–ˆâ–ˆâ–ˆ                              | 2125/23260 [6:29:10<64:32:34, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 17:15:51\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8292, 'grad_norm': 0.4737342298030853, 'learning_rate': 0.0001827171109200344, 'epoch': 1.827171109200344}\u001b[0m\n",
      "{'loss': 0.8292, 'grad_norm': 0.4737342298030853, 'learning_rate': 0.0001827171109200344, 'epoch': 1.83}\n",
      "  9%|â–ˆâ–ˆâ–ˆ                              | 2150/23260 [6:33:45<64:28:12, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 17:20:26\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8154, 'grad_norm': 0.3349239230155945, 'learning_rate': 0.00018486672398968185, 'epoch': 1.8486672398968187}\u001b[0m\n",
      "{'loss': 0.8154, 'grad_norm': 0.3349239230155945, 'learning_rate': 0.00018486672398968185, 'epoch': 1.85}\n",
      "  9%|â–ˆâ–ˆâ–ˆ                              | 2175/23260 [6:38:19<64:20:41, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 17:25:01\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8324, 'grad_norm': 0.4640583097934723, 'learning_rate': 0.00018701633705932934, 'epoch': 1.8701633705932932}\u001b[0m\n",
      "{'loss': 0.8324, 'grad_norm': 0.4640583097934723, 'learning_rate': 0.00018701633705932934, 'epoch': 1.87}\n",
      "  9%|â–ˆâ–ˆâ–ˆ                              | 2200/23260 [6:42:54<64:16:57, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 17:29:35\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7969, 'grad_norm': 0.3904768228530884, 'learning_rate': 0.0001891659501289768, 'epoch': 1.8916595012897677}\u001b[0m\n",
      "{'loss': 0.7969, 'grad_norm': 0.3904768228530884, 'learning_rate': 0.0001891659501289768, 'epoch': 1.89}\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–                             | 2225/23260 [6:47:29<64:12:47, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 17:34:10\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7893, 'grad_norm': 0.39979466795921326, 'learning_rate': 0.00019131556319862425, 'epoch': 1.9131556319862426}\u001b[0m\n",
      "{'loss': 0.7893, 'grad_norm': 0.39979466795921326, 'learning_rate': 0.00019131556319862425, 'epoch': 1.91}\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–                             | 2250/23260 [6:52:04<64:09:24, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 17:38:45\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.785, 'grad_norm': 0.40626510977745056, 'learning_rate': 0.0001934651762682717, 'epoch': 1.934651762682717}\u001b[0m\n",
      "{'loss': 0.785, 'grad_norm': 0.40626510977745056, 'learning_rate': 0.0001934651762682717, 'epoch': 1.93}\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–                             | 2275/23260 [6:56:39<64:06:07, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 17:43:20\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.816, 'grad_norm': 0.4626358151435852, 'learning_rate': 0.00019561478933791918, 'epoch': 1.9561478933791916}\u001b[0m\n",
      "{'loss': 0.816, 'grad_norm': 0.4626358151435852, 'learning_rate': 0.00019561478933791918, 'epoch': 1.96}\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–Ž                             | 2300/23260 [7:01:14<64:01:13, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 17:47:55\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.8497, 'grad_norm': 0.5893071889877319, 'learning_rate': 0.00019776440240756665, 'epoch': 1.9776440240756665}\u001b[0m\n",
      "{'loss': 0.8497, 'grad_norm': 0.5893071889877319, 'learning_rate': 0.00019776440240756665, 'epoch': 1.98}\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–Ž                             | 2325/23260 [7:05:49<63:57:18, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 17:52:30\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7904, 'grad_norm': 0.3767988681793213, 'learning_rate': 0.0001999140154772141, 'epoch': 1.999140154772141}\u001b[0m\n",
      "{'loss': 0.7904, 'grad_norm': 0.3767988681793213, 'learning_rate': 0.0001999140154772141, 'epoch': 2.0}\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–Ž                             | 2350/23260 [7:10:24<63:48:48, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 17:57:05\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6814, 'grad_norm': 0.4179143011569977, 'learning_rate': 0.00019977070793923763, 'epoch': 2.0206362854686155}\u001b[0m\n",
      "{'loss': 0.6814, 'grad_norm': 0.4179143011569977, 'learning_rate': 0.00019977070793923763, 'epoch': 2.02}\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–Ž                             | 2375/23260 [7:14:59<63:51:55, 11.01s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 18:01:40\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6978, 'grad_norm': 0.45258110761642456, 'learning_rate': 0.00019953186204261012, 'epoch': 2.0421324161650904}\u001b[0m\n",
      "{'loss': 0.6978, 'grad_norm': 0.45258110761642456, 'learning_rate': 0.00019953186204261012, 'epoch': 2.04}\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–                             | 2400/23260 [7:19:33<63:44:54, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 18:06:15\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6933, 'grad_norm': 0.5024971961975098, 'learning_rate': 0.00019929301614598263, 'epoch': 2.063628546861565}\u001b[0m\n",
      "{'loss': 0.6933, 'grad_norm': 0.5024971961975098, 'learning_rate': 0.00019929301614598263, 'epoch': 2.06}\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–                             | 2425/23260 [7:24:08<63:42:29, 11.01s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 18:10:50\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6957, 'grad_norm': 0.48528194427490234, 'learning_rate': 0.00019905417024935514, 'epoch': 2.0851246775580394}\u001b[0m\n",
      "{'loss': 0.6957, 'grad_norm': 0.48528194427490234, 'learning_rate': 0.00019905417024935514, 'epoch': 2.09}\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–                             | 2450/23260 [7:28:43<63:33:23, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 18:15:24\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6724, 'grad_norm': 0.47072672843933105, 'learning_rate': 0.00019881532435272762, 'epoch': 2.1066208082545144}\u001b[0m\n",
      "{'loss': 0.6724, 'grad_norm': 0.47072672843933105, 'learning_rate': 0.00019881532435272762, 'epoch': 2.11}\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–Œ                             | 2475/23260 [7:33:18<63:30:51, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 18:19:59\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6743, 'grad_norm': 0.5537104606628418, 'learning_rate': 0.00019857647845610013, 'epoch': 2.128116938950989}\u001b[0m\n",
      "{'loss': 0.6743, 'grad_norm': 0.5537104606628418, 'learning_rate': 0.00019857647845610013, 'epoch': 2.13}\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–Œ                             | 2500/23260 [7:37:53<63:23:21, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 18:24:34\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6884, 'grad_norm': 1.6107417345046997, 'learning_rate': 0.00019833763255947264, 'epoch': 2.1496130696474633}\u001b[0m\n",
      "{'loss': 0.6884, 'grad_norm': 1.6107417345046997, 'learning_rate': 0.00019833763255947264, 'epoch': 2.15}\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–Œ                             | 2525/23260 [7:42:28<63:22:00, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 18:29:09\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7012, 'grad_norm': 0.506607174873352, 'learning_rate': 0.00019809878666284515, 'epoch': 2.1711092003439383}\u001b[0m\n",
      "{'loss': 0.7012, 'grad_norm': 0.506607174873352, 'learning_rate': 0.00019809878666284515, 'epoch': 2.17}\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–Œ                             | 2550/23260 [7:47:03<63:19:11, 11.01s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 18:33:44\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7238, 'grad_norm': 0.505362331867218, 'learning_rate': 0.00019785994076621766, 'epoch': 2.1926053310404128}\u001b[0m\n",
      "{'loss': 0.7238, 'grad_norm': 0.505362331867218, 'learning_rate': 0.00019785994076621766, 'epoch': 2.19}\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–‹                             | 2575/23260 [7:51:38<63:14:12, 11.01s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 18:38:20\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.718, 'grad_norm': 0.45952242612838745, 'learning_rate': 0.00019762109486959014, 'epoch': 2.2141014617368873}\u001b[0m\n",
      "{'loss': 0.718, 'grad_norm': 0.45952242612838745, 'learning_rate': 0.00019762109486959014, 'epoch': 2.21}\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–‹                             | 2600/23260 [7:56:14<63:08:19, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 18:42:55\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7161, 'grad_norm': 0.5198662281036377, 'learning_rate': 0.00019738224897296265, 'epoch': 2.235597592433362}\u001b[0m\n",
      "{'loss': 0.7161, 'grad_norm': 0.5198662281036377, 'learning_rate': 0.00019738224897296265, 'epoch': 2.24}\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–‹                             | 2625/23260 [8:00:49<63:05:32, 11.01s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 18:47:30\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.708, 'grad_norm': 0.4352860152721405, 'learning_rate': 0.00019714340307633516, 'epoch': 2.2570937231298367}\u001b[0m\n",
      "{'loss': 0.708, 'grad_norm': 0.4352860152721405, 'learning_rate': 0.00019714340307633516, 'epoch': 2.26}\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–Š                             | 2650/23260 [8:05:24<62:59:11, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 18:52:05\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7075, 'grad_norm': 0.4979379177093506, 'learning_rate': 0.00019690455717970767, 'epoch': 2.278589853826311}\u001b[0m\n",
      "{'loss': 0.7075, 'grad_norm': 0.4979379177093506, 'learning_rate': 0.00019690455717970767, 'epoch': 2.28}\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–Š                             | 2675/23260 [8:09:59<62:56:38, 11.01s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 18:56:40\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6871, 'grad_norm': 0.4602571129798889, 'learning_rate': 0.00019666571128308015, 'epoch': 2.300085984522786}\u001b[0m\n",
      "{'loss': 0.6871, 'grad_norm': 0.4602571129798889, 'learning_rate': 0.00019666571128308015, 'epoch': 2.3}\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–Š                             | 2700/23260 [8:14:34<62:54:26, 11.01s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 19:01:15\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7194, 'grad_norm': 0.52537602186203, 'learning_rate': 0.00019642686538645266, 'epoch': 2.3215821152192606}\u001b[0m\n",
      "{'loss': 0.7194, 'grad_norm': 0.52537602186203, 'learning_rate': 0.00019642686538645266, 'epoch': 2.32}\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–Š                             | 2725/23260 [8:19:09<62:45:33, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 19:05:50\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7129, 'grad_norm': 0.5250939130783081, 'learning_rate': 0.00019618801948982517, 'epoch': 2.343078245915735}\u001b[0m\n",
      "{'loss': 0.7129, 'grad_norm': 0.5250939130783081, 'learning_rate': 0.00019618801948982517, 'epoch': 2.34}\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–‰                             | 2750/23260 [8:23:44<62:43:33, 11.01s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 19:10:26\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7027, 'grad_norm': 0.6912522315979004, 'learning_rate': 0.00019594917359319768, 'epoch': 2.36457437661221}\u001b[0m\n",
      "{'loss': 0.7027, 'grad_norm': 0.6912522315979004, 'learning_rate': 0.00019594917359319768, 'epoch': 2.36}\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–‰                             | 2775/23260 [8:28:20<62:38:12, 11.01s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 19:15:01\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7032, 'grad_norm': 0.4996841251850128, 'learning_rate': 0.0001957103276965702, 'epoch': 2.3860705073086845}\u001b[0m\n",
      "{'loss': 0.7032, 'grad_norm': 0.4996841251850128, 'learning_rate': 0.0001957103276965702, 'epoch': 2.39}\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–‰                             | 2800/23260 [8:32:55<62:32:18, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 19:19:36\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7102, 'grad_norm': 0.4777381718158722, 'learning_rate': 0.00019547148179994267, 'epoch': 2.407566638005159}\u001b[0m\n",
      "{'loss': 0.7102, 'grad_norm': 0.4777381718158722, 'learning_rate': 0.00019547148179994267, 'epoch': 2.41}\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆ                             | 2825/23260 [8:37:30<62:28:54, 11.01s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 19:24:11\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7014, 'grad_norm': 0.48079267144203186, 'learning_rate': 0.0001952326359033152, 'epoch': 2.4290627687016335}\u001b[0m\n",
      "{'loss': 0.7014, 'grad_norm': 0.48079267144203186, 'learning_rate': 0.0001952326359033152, 'epoch': 2.43}\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆ                             | 2850/23260 [8:42:05<62:24:55, 11.01s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 19:28:46\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7053, 'grad_norm': 0.43466827273368835, 'learning_rate': 0.0001949937900066877, 'epoch': 2.4505588993981084}\u001b[0m\n",
      "{'loss': 0.7053, 'grad_norm': 0.43466827273368835, 'learning_rate': 0.0001949937900066877, 'epoch': 2.45}\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆ                             | 2875/23260 [8:46:40<62:18:21, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 19:33:21\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7134, 'grad_norm': 0.5060872435569763, 'learning_rate': 0.00019475494411006018, 'epoch': 2.472055030094583}\u001b[0m\n",
      "{'loss': 0.7134, 'grad_norm': 0.5060872435569763, 'learning_rate': 0.00019475494411006018, 'epoch': 2.47}\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆ                             | 2900/23260 [8:51:15<62:11:58, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 19:37:56\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7004, 'grad_norm': 0.5252055525779724, 'learning_rate': 0.0001945160982134327, 'epoch': 2.4935511607910574}\u001b[0m\n",
      "{'loss': 0.7004, 'grad_norm': 0.5252055525779724, 'learning_rate': 0.0001945160982134327, 'epoch': 2.49}\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–                            | 2925/23260 [8:55:50<62:06:19, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 19:42:31\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6906, 'grad_norm': 0.4973471462726593, 'learning_rate': 0.0001942772523168052, 'epoch': 2.5150472914875324}\u001b[0m\n",
      "{'loss': 0.6906, 'grad_norm': 0.4973471462726593, 'learning_rate': 0.0001942772523168052, 'epoch': 2.52}\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–                            | 2950/23260 [9:00:25<62:01:35, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 19:47:06\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7097, 'grad_norm': 0.5637838840484619, 'learning_rate': 0.00019403840642017773, 'epoch': 2.536543422184007}\u001b[0m\n",
      "{'loss': 0.7097, 'grad_norm': 0.5637838840484619, 'learning_rate': 0.00019403840642017773, 'epoch': 2.54}\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–                            | 2975/23260 [9:05:00<61:55:29, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 19:51:41\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6776, 'grad_norm': 0.5437315106391907, 'learning_rate': 0.00019379956052355022, 'epoch': 2.5580395528804813}\u001b[0m\n",
      "{'loss': 0.6776, 'grad_norm': 0.5437315106391907, 'learning_rate': 0.00019379956052355022, 'epoch': 2.56}\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž                            | 3000/23260 [9:09:34<61:53:17, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 19:56:15\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7037, 'grad_norm': 0.48435696959495544, 'learning_rate': 0.00019356071462692272, 'epoch': 2.5795356835769563}\u001b[0m\n",
      "{'loss': 0.7037, 'grad_norm': 0.48435696959495544, 'learning_rate': 0.00019356071462692272, 'epoch': 2.58}\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž                            | 3025/23260 [9:14:09<61:46:57, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 20:00:50\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6829, 'grad_norm': 0.48692047595977783, 'learning_rate': 0.00019332186873029523, 'epoch': 2.6010318142734308}\u001b[0m\n",
      "{'loss': 0.6829, 'grad_norm': 0.48692047595977783, 'learning_rate': 0.00019332186873029523, 'epoch': 2.6}\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž                            | 3050/23260 [9:18:44<61:40:22, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 20:05:25\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6713, 'grad_norm': 0.4644424319267273, 'learning_rate': 0.00019308302283366772, 'epoch': 2.6225279449699053}\u001b[0m\n",
      "{'loss': 0.6713, 'grad_norm': 0.4644424319267273, 'learning_rate': 0.00019308302283366772, 'epoch': 2.62}\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž                            | 3075/23260 [9:23:19<61:36:57, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 20:10:00\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7162, 'grad_norm': 0.5528327822685242, 'learning_rate': 0.00019284417693704023, 'epoch': 2.64402407566638}\u001b[0m\n",
      "{'loss': 0.7162, 'grad_norm': 0.5528327822685242, 'learning_rate': 0.00019284417693704023, 'epoch': 2.64}\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–                            | 3100/23260 [9:27:54<61:35:09, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 20:14:35\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7045, 'grad_norm': 0.47358542680740356, 'learning_rate': 0.00019260533104041274, 'epoch': 2.6655202063628547}\u001b[0m\n",
      "{'loss': 0.7045, 'grad_norm': 0.47358542680740356, 'learning_rate': 0.00019260533104041274, 'epoch': 2.67}\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–                            | 3125/23260 [9:32:28<61:27:42, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 20:19:09\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6941, 'grad_norm': 0.5514109134674072, 'learning_rate': 0.00019236648514378525, 'epoch': 2.687016337059329}\u001b[0m\n",
      "{'loss': 0.6941, 'grad_norm': 0.5514109134674072, 'learning_rate': 0.00019236648514378525, 'epoch': 2.69}\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–                            | 3150/23260 [9:37:03<61:22:38, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 20:23:44\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7102, 'grad_norm': 0.5088763236999512, 'learning_rate': 0.00019212763924715776, 'epoch': 2.708512467755804}\u001b[0m\n",
      "{'loss': 0.7102, 'grad_norm': 0.5088763236999512, 'learning_rate': 0.00019212763924715776, 'epoch': 2.71}\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 3175/23260 [9:41:38<61:16:46, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 20:28:19\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7133, 'grad_norm': 0.5175427198410034, 'learning_rate': 0.00019188879335053024, 'epoch': 2.7300085984522786}\u001b[0m\n",
      "{'loss': 0.7133, 'grad_norm': 0.5175427198410034, 'learning_rate': 0.00019188879335053024, 'epoch': 2.73}\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 3200/23260 [9:46:13<61:15:56, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 20:32:54\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6995, 'grad_norm': 0.48700571060180664, 'learning_rate': 0.00019164994745390275, 'epoch': 2.751504729148753}\u001b[0m\n",
      "{'loss': 0.6995, 'grad_norm': 0.48700571060180664, 'learning_rate': 0.00019164994745390275, 'epoch': 2.75}\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 3225/23260 [9:50:47<61:09:09, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 20:37:28\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6741, 'grad_norm': 0.5436303615570068, 'learning_rate': 0.00019141110155727526, 'epoch': 2.7730008598452276}\u001b[0m\n",
      "{'loss': 0.6741, 'grad_norm': 0.5436303615570068, 'learning_rate': 0.00019141110155727526, 'epoch': 2.77}\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 3250/23260 [9:55:22<61:04:42, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 20:42:03\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7479, 'grad_norm': 0.48640862107276917, 'learning_rate': 0.00019117225566064777, 'epoch': 2.7944969905417025}\u001b[0m\n",
      "{'loss': 0.7479, 'grad_norm': 0.48640862107276917, 'learning_rate': 0.00019117225566064777, 'epoch': 2.79}\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 3275/23260 [9:59:57<60:59:32, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 20:46:38\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6959, 'grad_norm': 0.49009963870048523, 'learning_rate': 0.00019093340976402025, 'epoch': 2.815993121238177}\u001b[0m\n",
      "{'loss': 0.6959, 'grad_norm': 0.49009963870048523, 'learning_rate': 0.00019093340976402025, 'epoch': 2.82}\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 3300/23260 [10:04:32<60:57:30, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 20:51:13\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.705, 'grad_norm': 0.5507128238677979, 'learning_rate': 0.00019069456386739276, 'epoch': 2.8374892519346515}\u001b[0m\n",
      "{'loss': 0.705, 'grad_norm': 0.5507128238677979, 'learning_rate': 0.00019069456386739276, 'epoch': 2.84}\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 3325/23260 [10:09:06<60:51:45, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 20:55:48\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6808, 'grad_norm': 0.5469661355018616, 'learning_rate': 0.00019045571797076527, 'epoch': 2.8589853826311264}\u001b[0m\n",
      "{'loss': 0.6808, 'grad_norm': 0.5469661355018616, 'learning_rate': 0.00019045571797076527, 'epoch': 2.86}\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 3350/23260 [10:13:41<60:48:45, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 21:00:22\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6812, 'grad_norm': 0.5468399524688721, 'learning_rate': 0.00019021687207413778, 'epoch': 2.880481513327601}\u001b[0m\n",
      "{'loss': 0.6812, 'grad_norm': 0.5468399524688721, 'learning_rate': 0.00019021687207413778, 'epoch': 2.88}\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 3375/23260 [10:18:16<60:44:24, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 21:04:57\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7145, 'grad_norm': 0.5829960107803345, 'learning_rate': 0.0001899780261775103, 'epoch': 2.9019776440240754}\u001b[0m\n",
      "{'loss': 0.7145, 'grad_norm': 0.5829960107803345, 'learning_rate': 0.0001899780261775103, 'epoch': 2.9}\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 3387/23260 [10:20:28<60:40:26, 10.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 3400/23260 [10:22:51<60:38:32, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 21:09:32\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6941, 'grad_norm': 0.6018321514129639, 'learning_rate': 0.00018973918028088277, 'epoch': 2.9234737747205504}\u001b[0m\n",
      "{'loss': 0.6941, 'grad_norm': 0.6018321514129639, 'learning_rate': 0.00018973918028088277, 'epoch': 2.92}\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 3425/23260 [10:27:25<60:31:15, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 21:14:07\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7316, 'grad_norm': 0.5482103824615479, 'learning_rate': 0.0001895003343842553, 'epoch': 2.944969905417025}\u001b[0m\n",
      "{'loss': 0.7316, 'grad_norm': 0.5482103824615479, 'learning_rate': 0.0001895003343842553, 'epoch': 2.94}\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 3450/23260 [10:32:00<60:26:49, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 21:18:41\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.7135, 'grad_norm': 0.5509200692176819, 'learning_rate': 0.0001892614884876278, 'epoch': 2.9664660361134993}\u001b[0m\n",
      "{'loss': 0.7135, 'grad_norm': 0.5509200692176819, 'learning_rate': 0.0001892614884876278, 'epoch': 2.97}\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 3475/23260 [10:36:35<60:22:25, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 21:23:16\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6906, 'grad_norm': 0.5899989008903503, 'learning_rate': 0.00018902264259100027, 'epoch': 2.9879621668099743}\u001b[0m\n",
      "{'loss': 0.6906, 'grad_norm': 0.5899989008903503, 'learning_rate': 0.00018902264259100027, 'epoch': 2.99}\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 3500/23260 [10:41:10<60:16:34, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 21:27:51\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6203, 'grad_norm': 0.497906357049942, 'learning_rate': 0.0001887837966943728, 'epoch': 3.0094582975064488}\u001b[0m\n",
      "{'loss': 0.6203, 'grad_norm': 0.497906357049942, 'learning_rate': 0.0001887837966943728, 'epoch': 3.01}\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 3525/23260 [10:45:44<60:14:35, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 21:32:25\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5018, 'grad_norm': 0.6256920099258423, 'learning_rate': 0.0001885449507977453, 'epoch': 3.0309544282029233}\u001b[0m\n",
      "{'loss': 0.5018, 'grad_norm': 0.6256920099258423, 'learning_rate': 0.0001885449507977453, 'epoch': 3.03}\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                           | 3550/23260 [10:50:19<60:09:06, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 21:37:00\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.4864, 'grad_norm': 0.5793370604515076, 'learning_rate': 0.0001883061049011178, 'epoch': 3.052450558899398}\u001b[0m\n",
      "{'loss': 0.4864, 'grad_norm': 0.5793370604515076, 'learning_rate': 0.0001883061049011178, 'epoch': 3.05}\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                           | 3575/23260 [10:54:54<60:05:00, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 21:41:35\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.4955, 'grad_norm': 0.642516553401947, 'learning_rate': 0.00018806725900449031, 'epoch': 3.0739466895958727}\u001b[0m\n",
      "{'loss': 0.4955, 'grad_norm': 0.642516553401947, 'learning_rate': 0.00018806725900449031, 'epoch': 3.07}\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                           | 3600/23260 [10:59:28<59:59:12, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 21:46:10\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.471, 'grad_norm': 0.8643584251403809, 'learning_rate': 0.00018782841310786282, 'epoch': 3.095442820292347}\u001b[0m\n",
      "{'loss': 0.471, 'grad_norm': 0.8643584251403809, 'learning_rate': 0.00018782841310786282, 'epoch': 3.1}\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                           | 3625/23260 [11:04:03<59:58:34, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 21:50:44\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.4804, 'grad_norm': 0.6753399968147278, 'learning_rate': 0.00018758956721123533, 'epoch': 3.116938950988822}\u001b[0m\n",
      "{'loss': 0.4804, 'grad_norm': 0.6753399968147278, 'learning_rate': 0.00018758956721123533, 'epoch': 3.12}\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 3650/23260 [11:08:38<59:54:29, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 21:55:19\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.512, 'grad_norm': 0.6872609853744507, 'learning_rate': 0.00018735072131460782, 'epoch': 3.1384350816852966}\u001b[0m\n",
      "{'loss': 0.512, 'grad_norm': 0.6872609853744507, 'learning_rate': 0.00018735072131460782, 'epoch': 3.14}\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 3675/23260 [11:13:13<59:48:42, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 21:59:54\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.4849, 'grad_norm': 0.6466426849365234, 'learning_rate': 0.00018711187541798033, 'epoch': 3.159931212381771}\u001b[0m\n",
      "{'loss': 0.4849, 'grad_norm': 0.6466426849365234, 'learning_rate': 0.00018711187541798033, 'epoch': 3.16}\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 3700/23260 [11:17:47<59:41:11, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 22:04:28\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5173, 'grad_norm': 0.6661024689674377, 'learning_rate': 0.00018687302952135284, 'epoch': 3.181427343078246}\u001b[0m\n",
      "{'loss': 0.5173, 'grad_norm': 0.6661024689674377, 'learning_rate': 0.00018687302952135284, 'epoch': 3.18}\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 3725/23260 [11:22:22<59:36:02, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 22:09:03\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5248, 'grad_norm': 0.6065355539321899, 'learning_rate': 0.00018663418362472534, 'epoch': 3.2029234737747205}\u001b[0m\n",
      "{'loss': 0.5248, 'grad_norm': 0.6065355539321899, 'learning_rate': 0.00018663418362472534, 'epoch': 3.2}\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 3750/23260 [11:26:57<59:33:33, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 22:13:38\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5001, 'grad_norm': 0.6246384382247925, 'learning_rate': 0.00018639533772809783, 'epoch': 3.224419604471195}\u001b[0m\n",
      "{'loss': 0.5001, 'grad_norm': 0.6246384382247925, 'learning_rate': 0.00018639533772809783, 'epoch': 3.22}\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 3775/23260 [11:31:31<59:27:45, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 22:18:12\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.4921, 'grad_norm': 0.6469796895980835, 'learning_rate': 0.00018615649183147034, 'epoch': 3.24591573516767}\u001b[0m\n",
      "{'loss': 0.4921, 'grad_norm': 0.6469796895980835, 'learning_rate': 0.00018615649183147034, 'epoch': 3.25}\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 3800/23260 [11:36:06<59:22:34, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 22:22:47\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5019, 'grad_norm': 0.6907150745391846, 'learning_rate': 0.00018591764593484285, 'epoch': 3.2674118658641444}\u001b[0m\n",
      "{'loss': 0.5019, 'grad_norm': 0.6907150745391846, 'learning_rate': 0.00018591764593484285, 'epoch': 3.27}\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 3825/23260 [11:40:40<59:17:44, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 22:27:21\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.4933, 'grad_norm': 0.6651254296302795, 'learning_rate': 0.00018567880003821536, 'epoch': 3.288907996560619}\u001b[0m\n",
      "{'loss': 0.4933, 'grad_norm': 0.6651254296302795, 'learning_rate': 0.00018567880003821536, 'epoch': 3.29}\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 3850/23260 [11:45:15<59:14:46, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 22:31:56\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5107, 'grad_norm': 0.6790860891342163, 'learning_rate': 0.00018543995414158787, 'epoch': 3.310404127257094}\u001b[0m\n",
      "{'loss': 0.5107, 'grad_norm': 0.6790860891342163, 'learning_rate': 0.00018543995414158787, 'epoch': 3.31}\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 3875/23260 [11:49:50<59:11:45, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 22:36:31\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5307, 'grad_norm': 0.6800063848495483, 'learning_rate': 0.00018520110824496035, 'epoch': 3.3319002579535684}\u001b[0m\n",
      "{'loss': 0.5307, 'grad_norm': 0.6800063848495483, 'learning_rate': 0.00018520110824496035, 'epoch': 3.33}\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 3900/23260 [11:54:25<59:02:27, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 22:41:06\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5085, 'grad_norm': 0.6848032474517822, 'learning_rate': 0.00018496226234833286, 'epoch': 3.353396388650043}\u001b[0m\n",
      "{'loss': 0.5085, 'grad_norm': 0.6848032474517822, 'learning_rate': 0.00018496226234833286, 'epoch': 3.35}\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 3925/23260 [11:58:59<58:58:47, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 22:45:40\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5264, 'grad_norm': 0.6051310896873474, 'learning_rate': 0.00018472341645170537, 'epoch': 3.374892519346518}\u001b[0m\n",
      "{'loss': 0.5264, 'grad_norm': 0.6051310896873474, 'learning_rate': 0.00018472341645170537, 'epoch': 3.37}\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 3950/23260 [12:03:34<58:54:48, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 22:50:15\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5255, 'grad_norm': 0.8014066815376282, 'learning_rate': 0.00018448457055507785, 'epoch': 3.3963886500429923}\u001b[0m\n",
      "{'loss': 0.5255, 'grad_norm': 0.8014066815376282, 'learning_rate': 0.00018448457055507785, 'epoch': 3.4}\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 3975/23260 [12:08:08<58:47:13, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 22:54:49\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5023, 'grad_norm': 0.690574586391449, 'learning_rate': 0.0001842457246584504, 'epoch': 3.4178847807394668}\u001b[0m\n",
      "{'loss': 0.5023, 'grad_norm': 0.690574586391449, 'learning_rate': 0.0001842457246584504, 'epoch': 3.42}\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4000/23260 [12:12:43<58:45:30, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 22:59:24\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5201, 'grad_norm': 0.7221590280532837, 'learning_rate': 0.00018400687876182287, 'epoch': 3.4393809114359417}\u001b[0m\n",
      "{'loss': 0.5201, 'grad_norm': 0.7221590280532837, 'learning_rate': 0.00018400687876182287, 'epoch': 3.44}\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4025/23260 [12:17:17<58:43:04, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 23:03:59\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5133, 'grad_norm': 0.8178530931472778, 'learning_rate': 0.0001837680328651954, 'epoch': 3.460877042132416}\u001b[0m\n",
      "{'loss': 0.5133, 'grad_norm': 0.8178530931472778, 'learning_rate': 0.0001837680328651954, 'epoch': 3.46}\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4050/23260 [12:21:52<58:39:40, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 23:08:33\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.505, 'grad_norm': 0.6723678708076477, 'learning_rate': 0.0001835291869685679, 'epoch': 3.4823731728288907}\u001b[0m\n",
      "{'loss': 0.505, 'grad_norm': 0.6723678708076477, 'learning_rate': 0.0001835291869685679, 'epoch': 3.48}\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 4075/23260 [12:26:27<58:31:56, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 23:13:08\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5101, 'grad_norm': 0.7496854066848755, 'learning_rate': 0.00018329034107194037, 'epoch': 3.5038693035253656}\u001b[0m\n",
      "{'loss': 0.5101, 'grad_norm': 0.7496854066848755, 'learning_rate': 0.00018329034107194037, 'epoch': 3.5}\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 4100/23260 [12:31:01<58:26:17, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 23:17:42\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5076, 'grad_norm': 0.6597656607627869, 'learning_rate': 0.0001830514951753129, 'epoch': 3.52536543422184}\u001b[0m\n",
      "{'loss': 0.5076, 'grad_norm': 0.6597656607627869, 'learning_rate': 0.0001830514951753129, 'epoch': 3.53}\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 4125/23260 [12:35:36<58:22:05, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 23:22:17\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5285, 'grad_norm': 0.6975538730621338, 'learning_rate': 0.0001828126492786854, 'epoch': 3.5468615649183146}\u001b[0m\n",
      "{'loss': 0.5285, 'grad_norm': 0.6975538730621338, 'learning_rate': 0.0001828126492786854, 'epoch': 3.55}\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 4150/23260 [12:40:11<58:20:09, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 23:26:52\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5198, 'grad_norm': 0.6646653413772583, 'learning_rate': 0.0001825738033820579, 'epoch': 3.5683576956147895}\u001b[0m\n",
      "{'loss': 0.5198, 'grad_norm': 0.6646653413772583, 'learning_rate': 0.0001825738033820579, 'epoch': 3.57}\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 4175/23260 [12:44:45<58:12:24, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 23:31:26\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5049, 'grad_norm': 0.6594671010971069, 'learning_rate': 0.0001823349574854304, 'epoch': 3.589853826311264}\u001b[0m\n",
      "{'loss': 0.5049, 'grad_norm': 0.6594671010971069, 'learning_rate': 0.0001823349574854304, 'epoch': 3.59}\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 4200/23260 [12:49:20<58:09:32, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 23:36:01\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5259, 'grad_norm': 0.6359206438064575, 'learning_rate': 0.00018209611158880292, 'epoch': 3.6113499570077385}\u001b[0m\n",
      "{'loss': 0.5259, 'grad_norm': 0.6359206438064575, 'learning_rate': 0.00018209611158880292, 'epoch': 3.61}\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 4225/23260 [12:53:54<58:01:59, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 23:40:35\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5188, 'grad_norm': 0.7228837609291077, 'learning_rate': 0.00018185726569217543, 'epoch': 3.6328460877042135}\u001b[0m\n",
      "{'loss': 0.5188, 'grad_norm': 0.7228837609291077, 'learning_rate': 0.00018185726569217543, 'epoch': 3.63}\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 4250/23260 [12:58:29<57:57:47, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 23:45:10\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5422, 'grad_norm': 0.7474579811096191, 'learning_rate': 0.00018161841979554791, 'epoch': 3.654342218400688}\u001b[0m\n",
      "{'loss': 0.5422, 'grad_norm': 0.7474579811096191, 'learning_rate': 0.00018161841979554791, 'epoch': 3.65}\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 4275/23260 [13:03:03<57:54:11, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 23:49:44\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5162, 'grad_norm': 0.6336640119552612, 'learning_rate': 0.00018137957389892042, 'epoch': 3.6758383490971624}\u001b[0m\n",
      "{'loss': 0.5162, 'grad_norm': 0.6336640119552612, 'learning_rate': 0.00018137957389892042, 'epoch': 3.68}\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 4300/23260 [13:07:38<57:50:32, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 23:54:19\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5227, 'grad_norm': 0.7130029201507568, 'learning_rate': 0.00018114072800229293, 'epoch': 3.6973344797936374}\u001b[0m\n",
      "{'loss': 0.5227, 'grad_norm': 0.7130029201507568, 'learning_rate': 0.00018114072800229293, 'epoch': 3.7}\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 4325/23260 [13:12:13<57:48:38, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-02 23:58:54\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5342, 'grad_norm': 0.6803285479545593, 'learning_rate': 0.00018090188210566544, 'epoch': 3.718830610490112}\u001b[0m\n",
      "{'loss': 0.5342, 'grad_norm': 0.6803285479545593, 'learning_rate': 0.00018090188210566544, 'epoch': 3.72}\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 4350/23260 [13:16:47<57:41:42, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 00:03:28\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5366, 'grad_norm': 0.7344744205474854, 'learning_rate': 0.00018066303620903793, 'epoch': 3.7403267411865864}\u001b[0m\n",
      "{'loss': 0.5366, 'grad_norm': 0.7344744205474854, 'learning_rate': 0.00018066303620903793, 'epoch': 3.74}\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 4375/23260 [13:21:22<57:35:38, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 00:08:03\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5401, 'grad_norm': 0.6267384886741638, 'learning_rate': 0.00018042419031241044, 'epoch': 3.7618228718830613}\u001b[0m\n",
      "{'loss': 0.5401, 'grad_norm': 0.6267384886741638, 'learning_rate': 0.00018042419031241044, 'epoch': 3.76}\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 4400/23260 [13:25:56<57:29:42, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 00:12:37\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5345, 'grad_norm': 0.633733332157135, 'learning_rate': 0.00018018534441578295, 'epoch': 3.783319002579536}\u001b[0m\n",
      "{'loss': 0.5345, 'grad_norm': 0.633733332157135, 'learning_rate': 0.00018018534441578295, 'epoch': 3.78}\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 4425/23260 [13:30:31<57:29:10, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 00:17:12\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5171, 'grad_norm': 0.811829686164856, 'learning_rate': 0.00017994649851915546, 'epoch': 3.8048151332760103}\u001b[0m\n",
      "{'loss': 0.5171, 'grad_norm': 0.811829686164856, 'learning_rate': 0.00017994649851915546, 'epoch': 3.8}\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 4450/23260 [13:35:06<57:22:51, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 00:21:47\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5322, 'grad_norm': 0.6145852208137512, 'learning_rate': 0.00017970765262252797, 'epoch': 3.826311263972485}\u001b[0m\n",
      "{'loss': 0.5322, 'grad_norm': 0.6145852208137512, 'learning_rate': 0.00017970765262252797, 'epoch': 3.83}\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 4475/23260 [13:39:40<57:17:46, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 00:26:21\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5444, 'grad_norm': 0.6911201477050781, 'learning_rate': 0.00017946880672590045, 'epoch': 3.8478073946689597}\u001b[0m\n",
      "{'loss': 0.5444, 'grad_norm': 0.6911201477050781, 'learning_rate': 0.00017946880672590045, 'epoch': 3.85}\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 4500/23260 [13:44:15<57:14:57, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 00:30:56\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5223, 'grad_norm': 0.6805303692817688, 'learning_rate': 0.00017922996082927296, 'epoch': 3.869303525365434}\u001b[0m\n",
      "{'loss': 0.5223, 'grad_norm': 0.6805303692817688, 'learning_rate': 0.00017922996082927296, 'epoch': 3.87}\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 4525/23260 [13:48:49<57:08:47, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 00:35:30\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5454, 'grad_norm': 0.6718162894248962, 'learning_rate': 0.00017899111493264547, 'epoch': 3.8907996560619087}\u001b[0m\n",
      "{'loss': 0.5454, 'grad_norm': 0.6718162894248962, 'learning_rate': 0.00017899111493264547, 'epoch': 3.89}\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 4550/23260 [13:53:24<57:04:52, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 00:40:05\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.547, 'grad_norm': 0.8510775566101074, 'learning_rate': 0.00017875226903601795, 'epoch': 3.9122957867583836}\u001b[0m\n",
      "{'loss': 0.547, 'grad_norm': 0.8510775566101074, 'learning_rate': 0.00017875226903601795, 'epoch': 3.91}\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 4575/23260 [13:57:59<57:06:17, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 00:44:40\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5227, 'grad_norm': 0.7558218240737915, 'learning_rate': 0.0001785134231393905, 'epoch': 3.933791917454858}\u001b[0m\n",
      "{'loss': 0.5227, 'grad_norm': 0.7558218240737915, 'learning_rate': 0.0001785134231393905, 'epoch': 3.93}\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 4600/23260 [14:02:33<56:58:45, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 00:49:14\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5324, 'grad_norm': 0.7269992828369141, 'learning_rate': 0.00017827457724276297, 'epoch': 3.9552880481513326}\u001b[0m\n",
      "{'loss': 0.5324, 'grad_norm': 0.7269992828369141, 'learning_rate': 0.00017827457724276297, 'epoch': 3.96}\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 4625/23260 [14:07:08<56:52:03, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 00:53:49\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5261, 'grad_norm': 0.7108991146087646, 'learning_rate': 0.00017803573134613548, 'epoch': 3.9767841788478075}\u001b[0m\n",
      "{'loss': 0.5261, 'grad_norm': 0.7108991146087646, 'learning_rate': 0.00017803573134613548, 'epoch': 3.98}\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 4650/23260 [14:11:43<56:48:25, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 00:58:24\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.5153, 'grad_norm': 0.7939459681510925, 'learning_rate': 0.000177796885449508, 'epoch': 3.998280309544282}\u001b[0m\n",
      "{'loss': 0.5153, 'grad_norm': 0.7939459681510925, 'learning_rate': 0.000177796885449508, 'epoch': 4.0}\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 4675/23260 [14:16:17<56:43:26, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 01:02:58\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3399, 'grad_norm': 0.6708583831787109, 'learning_rate': 0.00017755803955288047, 'epoch': 4.019776440240757}\u001b[0m\n",
      "{'loss': 0.3399, 'grad_norm': 0.6708583831787109, 'learning_rate': 0.00017755803955288047, 'epoch': 4.02}\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 4700/23260 [14:20:52<56:37:51, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 01:07:33\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3243, 'grad_norm': 0.8987040519714355, 'learning_rate': 0.000177319193656253, 'epoch': 4.041272570937231}\u001b[0m\n",
      "{'loss': 0.3243, 'grad_norm': 0.8987040519714355, 'learning_rate': 0.000177319193656253, 'epoch': 4.04}\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 4725/23260 [14:25:26<56:32:40, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 01:12:08\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3203, 'grad_norm': 0.7548750638961792, 'learning_rate': 0.0001770803477596255, 'epoch': 4.062768701633706}\u001b[0m\n",
      "{'loss': 0.3203, 'grad_norm': 0.7548750638961792, 'learning_rate': 0.0001770803477596255, 'epoch': 4.06}\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 4750/23260 [14:30:01<56:28:48, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 01:16:42\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3329, 'grad_norm': 0.824444591999054, 'learning_rate': 0.000176841501862998, 'epoch': 4.084264832330181}\u001b[0m\n",
      "{'loss': 0.3329, 'grad_norm': 0.824444591999054, 'learning_rate': 0.000176841501862998, 'epoch': 4.08}\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 4775/23260 [14:34:36<56:24:01, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 01:21:17\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3281, 'grad_norm': 0.8192087411880493, 'learning_rate': 0.0001766026559663705, 'epoch': 4.105760963026655}\u001b[0m\n",
      "{'loss': 0.3281, 'grad_norm': 0.8192087411880493, 'learning_rate': 0.0001766026559663705, 'epoch': 4.11}\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 4800/23260 [14:39:10<56:20:54, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 01:25:51\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3374, 'grad_norm': 0.6980330348014832, 'learning_rate': 0.00017636381006974302, 'epoch': 4.12725709372313}\u001b[0m\n",
      "{'loss': 0.3374, 'grad_norm': 0.6980330348014832, 'learning_rate': 0.00017636381006974302, 'epoch': 4.13}\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 4825/23260 [14:43:45<56:13:18, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 01:30:26\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3261, 'grad_norm': 0.9189237356185913, 'learning_rate': 0.00017612496417311553, 'epoch': 4.148753224419605}\u001b[0m\n",
      "{'loss': 0.3261, 'grad_norm': 0.9189237356185913, 'learning_rate': 0.00017612496417311553, 'epoch': 4.15}\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 4850/23260 [14:48:19<56:06:02, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 01:35:00\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3257, 'grad_norm': 0.7778869271278381, 'learning_rate': 0.000175886118276488, 'epoch': 4.170249355116079}\u001b[0m\n",
      "{'loss': 0.3257, 'grad_norm': 0.7778869271278381, 'learning_rate': 0.000175886118276488, 'epoch': 4.17}\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 4875/23260 [14:52:54<56:04:17, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 01:39:35\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.328, 'grad_norm': 0.8300151228904724, 'learning_rate': 0.00017564727237986052, 'epoch': 4.191745485812554}\u001b[0m\n",
      "{'loss': 0.328, 'grad_norm': 0.8300151228904724, 'learning_rate': 0.00017564727237986052, 'epoch': 4.19}\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 4900/23260 [14:57:28<55:58:24, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 01:44:10\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3308, 'grad_norm': 0.9009163975715637, 'learning_rate': 0.00017540842648323303, 'epoch': 4.213241616509029}\u001b[0m\n",
      "{'loss': 0.3308, 'grad_norm': 0.9009163975715637, 'learning_rate': 0.00017540842648323303, 'epoch': 4.21}\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 4925/23260 [15:02:03<55:55:03, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 01:48:44\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3556, 'grad_norm': 0.774556577205658, 'learning_rate': 0.00017516958058660554, 'epoch': 4.234737747205503}\u001b[0m\n",
      "{'loss': 0.3556, 'grad_norm': 0.774556577205658, 'learning_rate': 0.00017516958058660554, 'epoch': 4.23}\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 4950/23260 [15:06:37<55:50:55, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 01:53:19\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3358, 'grad_norm': 0.7690508961677551, 'learning_rate': 0.00017493073468997802, 'epoch': 4.256233877901978}\u001b[0m\n",
      "{'loss': 0.3358, 'grad_norm': 0.7690508961677551, 'learning_rate': 0.00017493073468997802, 'epoch': 4.26}\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 4975/23260 [15:11:12<55:47:38, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 01:57:53\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3304, 'grad_norm': 0.7740687727928162, 'learning_rate': 0.00017469188879335053, 'epoch': 4.277730008598453}\u001b[0m\n",
      "{'loss': 0.3304, 'grad_norm': 0.7740687727928162, 'learning_rate': 0.00017469188879335053, 'epoch': 4.28}\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 5000/23260 [15:15:46<55:40:15, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 02:02:27\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3502, 'grad_norm': 0.8525468111038208, 'learning_rate': 0.00017445304289672304, 'epoch': 4.299226139294927}\u001b[0m\n",
      "{'loss': 0.3502, 'grad_norm': 0.8525468111038208, 'learning_rate': 0.00017445304289672304, 'epoch': 4.3}\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 5025/23260 [15:20:21<55:39:22, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 02:07:02\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3356, 'grad_norm': 0.9331154823303223, 'learning_rate': 0.00017421419700009553, 'epoch': 4.320722269991402}\u001b[0m\n",
      "{'loss': 0.3356, 'grad_norm': 0.9331154823303223, 'learning_rate': 0.00017421419700009553, 'epoch': 4.32}\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 5050/23260 [15:24:56<55:34:41, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 02:11:37\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3513, 'grad_norm': 0.8821337223052979, 'learning_rate': 0.00017397535110346806, 'epoch': 4.342218400687877}\u001b[0m\n",
      "{'loss': 0.3513, 'grad_norm': 0.8821337223052979, 'learning_rate': 0.00017397535110346806, 'epoch': 4.34}\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 5075/23260 [15:29:30<55:29:12, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 02:16:11\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3443, 'grad_norm': 0.9627164006233215, 'learning_rate': 0.00017373650520684055, 'epoch': 4.363714531384351}\u001b[0m\n",
      "{'loss': 0.3443, 'grad_norm': 0.9627164006233215, 'learning_rate': 0.00017373650520684055, 'epoch': 4.36}\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 5100/23260 [15:34:05<55:27:20, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 02:20:46\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3434, 'grad_norm': 0.7424247860908508, 'learning_rate': 0.00017349765931021306, 'epoch': 4.3852106620808255}\u001b[0m\n",
      "{'loss': 0.3434, 'grad_norm': 0.7424247860908508, 'learning_rate': 0.00017349765931021306, 'epoch': 4.39}\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 5125/23260 [15:38:40<55:20:50, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 02:25:21\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3665, 'grad_norm': 0.9852449297904968, 'learning_rate': 0.00017325881341358557, 'epoch': 4.4067067927773005}\u001b[0m\n",
      "{'loss': 0.3665, 'grad_norm': 0.9852449297904968, 'learning_rate': 0.00017325881341358557, 'epoch': 4.41}\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 5150/23260 [15:43:14<55:15:17, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 02:29:55\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3464, 'grad_norm': 0.7092613577842712, 'learning_rate': 0.00017301996751695805, 'epoch': 4.4282029234737745}\u001b[0m\n",
      "{'loss': 0.3464, 'grad_norm': 0.7092613577842712, 'learning_rate': 0.00017301996751695805, 'epoch': 4.43}\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 5175/23260 [15:47:49<55:12:35, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 02:34:30\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3546, 'grad_norm': 0.7889679074287415, 'learning_rate': 0.00017278112162033059, 'epoch': 4.4496990541702495}\u001b[0m\n",
      "{'loss': 0.3546, 'grad_norm': 0.7889679074287415, 'learning_rate': 0.00017278112162033059, 'epoch': 4.45}\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 5200/23260 [15:52:24<55:06:17, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 02:39:05\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.353, 'grad_norm': 0.8294398188591003, 'learning_rate': 0.00017254227572370307, 'epoch': 4.471195184866724}\u001b[0m\n",
      "{'loss': 0.353, 'grad_norm': 0.8294398188591003, 'learning_rate': 0.00017254227572370307, 'epoch': 4.47}\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 5225/23260 [15:56:58<55:02:13, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 02:43:39\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3524, 'grad_norm': 1.0244463682174683, 'learning_rate': 0.00017230342982707558, 'epoch': 4.4926913155631985}\u001b[0m\n",
      "{'loss': 0.3524, 'grad_norm': 1.0244463682174683, 'learning_rate': 0.00017230342982707558, 'epoch': 4.49}\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 5250/23260 [16:01:33<54:57:23, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 02:48:14\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3574, 'grad_norm': 0.9150568246841431, 'learning_rate': 0.0001720645839304481, 'epoch': 4.514187446259673}\u001b[0m\n",
      "{'loss': 0.3574, 'grad_norm': 0.9150568246841431, 'learning_rate': 0.0001720645839304481, 'epoch': 4.51}\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 5275/23260 [16:06:08<54:51:48, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 02:52:49\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3559, 'grad_norm': 0.7930843830108643, 'learning_rate': 0.00017182573803382057, 'epoch': 4.535683576956147}\u001b[0m\n",
      "{'loss': 0.3559, 'grad_norm': 0.7930843830108643, 'learning_rate': 0.00017182573803382057, 'epoch': 4.54}\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 5300/23260 [16:10:42<54:51:26, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 02:57:23\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3505, 'grad_norm': 0.7720821499824524, 'learning_rate': 0.0001715868921371931, 'epoch': 4.557179707652622}\u001b[0m\n",
      "{'loss': 0.3505, 'grad_norm': 0.7720821499824524, 'learning_rate': 0.0001715868921371931, 'epoch': 4.56}\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 5325/23260 [16:15:17<54:46:22, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 03:01:58\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3635, 'grad_norm': 1.1418988704681396, 'learning_rate': 0.0001713480462405656, 'epoch': 4.578675838349097}\u001b[0m\n",
      "{'loss': 0.3635, 'grad_norm': 1.1418988704681396, 'learning_rate': 0.0001713480462405656, 'epoch': 4.58}\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 5350/23260 [16:19:52<54:40:47, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 03:06:33\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3609, 'grad_norm': 0.8777328729629517, 'learning_rate': 0.0001711092003439381, 'epoch': 4.600171969045572}\u001b[0m\n",
      "{'loss': 0.3609, 'grad_norm': 0.8777328729629517, 'learning_rate': 0.0001711092003439381, 'epoch': 4.6}\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 5375/23260 [16:24:26<54:34:20, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 03:11:07\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.35, 'grad_norm': 0.6907481551170349, 'learning_rate': 0.0001708703544473106, 'epoch': 4.621668099742046}\u001b[0m\n",
      "{'loss': 0.35, 'grad_norm': 0.6907481551170349, 'learning_rate': 0.0001708703544473106, 'epoch': 4.62}\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 5400/23260 [16:29:01<54:28:06, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 03:15:42\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3551, 'grad_norm': 0.8467909693717957, 'learning_rate': 0.00017063150855068312, 'epoch': 4.643164230438521}\u001b[0m\n",
      "{'loss': 0.3551, 'grad_norm': 0.8467909693717957, 'learning_rate': 0.00017063150855068312, 'epoch': 4.64}\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 5425/23260 [16:33:35<54:24:02, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 03:20:16\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3461, 'grad_norm': 0.6623135209083557, 'learning_rate': 0.0001703926626540556, 'epoch': 4.664660361134995}\u001b[0m\n",
      "{'loss': 0.3461, 'grad_norm': 0.6623135209083557, 'learning_rate': 0.0001703926626540556, 'epoch': 4.66}\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 5450/23260 [16:38:10<54:20:09, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 03:24:51\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3655, 'grad_norm': 1.051330327987671, 'learning_rate': 0.0001701538167574281, 'epoch': 4.68615649183147}\u001b[0m\n",
      "{'loss': 0.3655, 'grad_norm': 1.051330327987671, 'learning_rate': 0.0001701538167574281, 'epoch': 4.69}\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 5475/23260 [16:42:44<54:15:25, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 03:29:26\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3556, 'grad_norm': 0.9295716881752014, 'learning_rate': 0.00016991497086080062, 'epoch': 4.707652622527945}\u001b[0m\n",
      "{'loss': 0.3556, 'grad_norm': 0.9295716881752014, 'learning_rate': 0.00016991497086080062, 'epoch': 4.71}\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 5500/23260 [16:47:19<54:09:04, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 03:34:00\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3605, 'grad_norm': 0.8101497292518616, 'learning_rate': 0.00016967612496417313, 'epoch': 4.72914875322442}\u001b[0m\n",
      "{'loss': 0.3605, 'grad_norm': 0.8101497292518616, 'learning_rate': 0.00016967612496417313, 'epoch': 4.73}\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 5525/23260 [16:51:54<54:05:54, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 03:38:35\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3767, 'grad_norm': 0.8216223120689392, 'learning_rate': 0.00016943727906754564, 'epoch': 4.750644883920894}\u001b[0m\n",
      "{'loss': 0.3767, 'grad_norm': 0.8216223120689392, 'learning_rate': 0.00016943727906754564, 'epoch': 4.75}\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 5550/23260 [16:56:28<53:58:46, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 03:43:09\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3687, 'grad_norm': 0.7374293208122253, 'learning_rate': 0.00016919843317091812, 'epoch': 4.772141014617369}\u001b[0m\n",
      "{'loss': 0.3687, 'grad_norm': 0.7374293208122253, 'learning_rate': 0.00016919843317091812, 'epoch': 4.77}\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 5575/23260 [17:01:02<53:55:55, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 03:47:43\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3742, 'grad_norm': 0.9946215748786926, 'learning_rate': 0.00016895958727429063, 'epoch': 4.793637145313843}\u001b[0m\n",
      "{'loss': 0.3742, 'grad_norm': 0.9946215748786926, 'learning_rate': 0.00016895958727429063, 'epoch': 4.79}\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 5600/23260 [17:05:37<53:51:08, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 03:52:18\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3702, 'grad_norm': 0.8960615396499634, 'learning_rate': 0.00016872074137766314, 'epoch': 4.815133276010318}\u001b[0m\n",
      "{'loss': 0.3702, 'grad_norm': 0.8960615396499634, 'learning_rate': 0.00016872074137766314, 'epoch': 4.82}\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 5625/23260 [17:10:11<53:47:23, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 03:56:52\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3632, 'grad_norm': 0.8471852540969849, 'learning_rate': 0.00016848189548103562, 'epoch': 4.836629406706793}\u001b[0m\n",
      "{'loss': 0.3632, 'grad_norm': 0.8471852540969849, 'learning_rate': 0.00016848189548103562, 'epoch': 4.84}\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 5650/23260 [17:14:46<53:40:22, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 04:01:27\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3779, 'grad_norm': 0.8566226363182068, 'learning_rate': 0.00016824304958440816, 'epoch': 4.858125537403267}\u001b[0m\n",
      "{'loss': 0.3779, 'grad_norm': 0.8566226363182068, 'learning_rate': 0.00016824304958440816, 'epoch': 4.86}\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 5675/23260 [17:19:20<53:37:04, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 04:06:01\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.357, 'grad_norm': 0.7783130407333374, 'learning_rate': 0.00016800420368778064, 'epoch': 4.879621668099742}\u001b[0m\n",
      "{'loss': 0.357, 'grad_norm': 0.7783130407333374, 'learning_rate': 0.00016800420368778064, 'epoch': 4.88}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 5700/23260 [17:23:54<53:32:06, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 04:10:36\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3634, 'grad_norm': 0.815925121307373, 'learning_rate': 0.00016776535779115315, 'epoch': 4.901117798796217}\u001b[0m\n",
      "{'loss': 0.3634, 'grad_norm': 0.815925121307373, 'learning_rate': 0.00016776535779115315, 'epoch': 4.9}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 5725/23260 [17:28:29<53:26:23, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 04:15:10\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3776, 'grad_norm': 0.8141160011291504, 'learning_rate': 0.00016752651189452566, 'epoch': 4.922613929492691}\u001b[0m\n",
      "{'loss': 0.3776, 'grad_norm': 0.8141160011291504, 'learning_rate': 0.00016752651189452566, 'epoch': 4.92}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 5750/23260 [17:33:03<53:22:18, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 04:19:44\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3853, 'grad_norm': 0.8244337439537048, 'learning_rate': 0.00016728766599789815, 'epoch': 4.944110060189166}\u001b[0m\n",
      "{'loss': 0.3853, 'grad_norm': 0.8244337439537048, 'learning_rate': 0.00016728766599789815, 'epoch': 4.94}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 5775/23260 [17:37:38<53:19:33, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 04:24:19\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3644, 'grad_norm': 0.8213483095169067, 'learning_rate': 0.00016704882010127068, 'epoch': 4.965606190885641}\u001b[0m\n",
      "{'loss': 0.3644, 'grad_norm': 0.8213483095169067, 'learning_rate': 0.00016704882010127068, 'epoch': 4.97}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 5800/23260 [17:42:12<53:22:10, 11.00s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 04:28:54\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3721, 'grad_norm': 0.8086991310119629, 'learning_rate': 0.00016680997420464317, 'epoch': 4.987102321582115}\u001b[0m\n",
      "{'loss': 0.3721, 'grad_norm': 0.8086991310119629, 'learning_rate': 0.00016680997420464317, 'epoch': 4.99}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 5825/23260 [17:46:47<53:10:27, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 04:33:28\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.3105, 'grad_norm': 0.8000922799110413, 'learning_rate': 0.00016657112830801568, 'epoch': 5.00859845227859}\u001b[0m\n",
      "{'loss': 0.3105, 'grad_norm': 0.8000922799110413, 'learning_rate': 0.00016657112830801568, 'epoch': 5.01}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 5850/23260 [17:51:21<53:03:44, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 04:38:02\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.1977, 'grad_norm': 0.7670583724975586, 'learning_rate': 0.00016633228241138819, 'epoch': 5.030094582975065}\u001b[0m\n",
      "{'loss': 0.1977, 'grad_norm': 0.7670583724975586, 'learning_rate': 0.00016633228241138819, 'epoch': 5.03}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 5875/23260 [17:55:56<53:02:49, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 04:42:37\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2008, 'grad_norm': 0.7807949781417847, 'learning_rate': 0.00016609343651476067, 'epoch': 5.051590713671539}\u001b[0m\n",
      "{'loss': 0.2008, 'grad_norm': 0.7807949781417847, 'learning_rate': 0.00016609343651476067, 'epoch': 5.05}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 5900/23260 [18:00:30<52:57:30, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 04:47:11\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2028, 'grad_norm': 0.902736485004425, 'learning_rate': 0.0001658545906181332, 'epoch': 5.073086844368014}\u001b[0m\n",
      "{'loss': 0.2028, 'grad_norm': 0.902736485004425, 'learning_rate': 0.0001658545906181332, 'epoch': 5.07}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 5925/23260 [18:05:05<52:52:26, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 04:51:46\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2017, 'grad_norm': 0.8826754093170166, 'learning_rate': 0.0001656157447215057, 'epoch': 5.094582975064489}\u001b[0m\n",
      "{'loss': 0.2017, 'grad_norm': 0.8826754093170166, 'learning_rate': 0.0001656157447215057, 'epoch': 5.09}\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 5950/23260 [18:09:39<52:46:55, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 04:56:20\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2039, 'grad_norm': 0.76314777135849, 'learning_rate': 0.0001653768988248782, 'epoch': 5.116079105760963}\u001b[0m\n",
      "{'loss': 0.2039, 'grad_norm': 0.76314777135849, 'learning_rate': 0.0001653768988248782, 'epoch': 5.12}\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 5975/23260 [18:14:14<52:44:32, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 05:00:55\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2041, 'grad_norm': 0.9575319886207581, 'learning_rate': 0.0001651380529282507, 'epoch': 5.137575236457438}\u001b[0m\n",
      "{'loss': 0.2041, 'grad_norm': 0.9575319886207581, 'learning_rate': 0.0001651380529282507, 'epoch': 5.14}\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                       | 6000/23260 [18:18:48<52:39:17, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 05:05:29\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.206, 'grad_norm': 0.7849698662757874, 'learning_rate': 0.00016489920703162322, 'epoch': 5.159071367153913}\u001b[0m\n",
      "{'loss': 0.206, 'grad_norm': 0.7849698662757874, 'learning_rate': 0.00016489920703162322, 'epoch': 5.16}\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                       | 6025/23260 [18:23:23<52:35:52, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 05:10:04\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2077, 'grad_norm': 0.8849787712097168, 'learning_rate': 0.0001646603611349957, 'epoch': 5.180567497850387}\u001b[0m\n",
      "{'loss': 0.2077, 'grad_norm': 0.8849787712097168, 'learning_rate': 0.0001646603611349957, 'epoch': 5.18}\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                       | 6050/23260 [18:27:57<52:30:44, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 05:14:38\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2087, 'grad_norm': 0.9279953241348267, 'learning_rate': 0.0001644215152383682, 'epoch': 5.2020636285468616}\u001b[0m\n",
      "{'loss': 0.2087, 'grad_norm': 0.9279953241348267, 'learning_rate': 0.0001644215152383682, 'epoch': 5.2}\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                       | 6075/23260 [18:32:32<52:24:40, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 05:19:13\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2091, 'grad_norm': 0.9225236773490906, 'learning_rate': 0.00016418266934174072, 'epoch': 5.2235597592433365}\u001b[0m\n",
      "{'loss': 0.2091, 'grad_norm': 0.9225236773490906, 'learning_rate': 0.00016418266934174072, 'epoch': 5.22}\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 6100/23260 [18:37:06<52:20:04, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 05:23:48\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2087, 'grad_norm': 0.9083413481712341, 'learning_rate': 0.00016394382344511323, 'epoch': 5.2450558899398105}\u001b[0m\n",
      "{'loss': 0.2087, 'grad_norm': 0.9083413481712341, 'learning_rate': 0.00016394382344511323, 'epoch': 5.25}\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 6125/23260 [18:41:41<52:15:18, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 05:28:22\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2042, 'grad_norm': 0.851170539855957, 'learning_rate': 0.00016370497754848574, 'epoch': 5.2665520206362855}\u001b[0m\n",
      "{'loss': 0.2042, 'grad_norm': 0.851170539855957, 'learning_rate': 0.00016370497754848574, 'epoch': 5.27}\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 6150/23260 [18:46:15<52:11:21, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 05:32:57\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.212, 'grad_norm': 0.7882429957389832, 'learning_rate': 0.00016346613165185822, 'epoch': 5.28804815133276}\u001b[0m\n",
      "{'loss': 0.212, 'grad_norm': 0.7882429957389832, 'learning_rate': 0.00016346613165185822, 'epoch': 5.29}\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 6175/23260 [18:50:50<52:04:53, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 05:37:31\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2117, 'grad_norm': 0.7729765772819519, 'learning_rate': 0.00016322728575523073, 'epoch': 5.3095442820292345}\u001b[0m\n",
      "{'loss': 0.2117, 'grad_norm': 0.7729765772819519, 'learning_rate': 0.00016322728575523073, 'epoch': 5.31}\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 6200/23260 [18:55:24<52:00:28, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 05:42:06\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2239, 'grad_norm': 1.026593804359436, 'learning_rate': 0.00016298843985860324, 'epoch': 5.331040412725709}\u001b[0m\n",
      "{'loss': 0.2239, 'grad_norm': 1.026593804359436, 'learning_rate': 0.00016298843985860324, 'epoch': 5.33}\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 6225/23260 [18:59:59<51:54:56, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 05:46:40\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2134, 'grad_norm': 0.7679502964019775, 'learning_rate': 0.00016274959396197572, 'epoch': 5.352536543422184}\u001b[0m\n",
      "{'loss': 0.2134, 'grad_norm': 0.7679502964019775, 'learning_rate': 0.00016274959396197572, 'epoch': 5.35}\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 6250/23260 [19:04:33<51:52:45, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 05:51:14\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.219, 'grad_norm': 0.6477653384208679, 'learning_rate': 0.00016251074806534826, 'epoch': 5.374032674118658}\u001b[0m\n",
      "{'loss': 0.219, 'grad_norm': 0.6477653384208679, 'learning_rate': 0.00016251074806534826, 'epoch': 5.37}\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 6275/23260 [19:09:08<51:45:25, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 05:55:49\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2269, 'grad_norm': 0.7920289039611816, 'learning_rate': 0.00016227190216872074, 'epoch': 5.395528804815133}\u001b[0m\n",
      "{'loss': 0.2269, 'grad_norm': 0.7920289039611816, 'learning_rate': 0.00016227190216872074, 'epoch': 5.4}\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 6300/23260 [19:13:42<51:40:58, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 06:00:23\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2122, 'grad_norm': 0.868735134601593, 'learning_rate': 0.00016203305627209325, 'epoch': 5.417024935511608}\u001b[0m\n",
      "{'loss': 0.2122, 'grad_norm': 0.868735134601593, 'learning_rate': 0.00016203305627209325, 'epoch': 5.42}\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 6325/23260 [19:18:16<51:38:21, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 06:04:58\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2236, 'grad_norm': 0.864105224609375, 'learning_rate': 0.00016179421037546576, 'epoch': 5.438521066208082}\u001b[0m\n",
      "{'loss': 0.2236, 'grad_norm': 0.864105224609375, 'learning_rate': 0.00016179421037546576, 'epoch': 5.44}\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 6350/23260 [19:22:51<51:37:23, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 06:09:32\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2227, 'grad_norm': 0.8370702266693115, 'learning_rate': 0.00016155536447883825, 'epoch': 5.460017196904557}\u001b[0m\n",
      "{'loss': 0.2227, 'grad_norm': 0.8370702266693115, 'learning_rate': 0.00016155536447883825, 'epoch': 5.46}\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 6375/23260 [19:27:26<51:30:41, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 06:14:07\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2151, 'grad_norm': 0.8107748031616211, 'learning_rate': 0.00016131651858221078, 'epoch': 5.481513327601032}\u001b[0m\n",
      "{'loss': 0.2151, 'grad_norm': 0.8107748031616211, 'learning_rate': 0.00016131651858221078, 'epoch': 5.48}\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 6400/23260 [19:32:00<51:23:32, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 06:18:41\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2207, 'grad_norm': 0.8683745861053467, 'learning_rate': 0.00016107767268558326, 'epoch': 5.503009458297506}\u001b[0m\n",
      "{'loss': 0.2207, 'grad_norm': 0.8683745861053467, 'learning_rate': 0.00016107767268558326, 'epoch': 5.5}\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 6425/23260 [19:36:34<51:19:28, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 06:23:16\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2279, 'grad_norm': 0.8248339295387268, 'learning_rate': 0.00016083882678895577, 'epoch': 5.524505588993981}\u001b[0m\n",
      "{'loss': 0.2279, 'grad_norm': 0.8248339295387268, 'learning_rate': 0.00016083882678895577, 'epoch': 5.52}\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 6450/23260 [19:41:09<51:15:48, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 06:27:50\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2187, 'grad_norm': 0.8099578022956848, 'learning_rate': 0.00016059998089232828, 'epoch': 5.546001719690456}\u001b[0m\n",
      "{'loss': 0.2187, 'grad_norm': 0.8099578022956848, 'learning_rate': 0.00016059998089232828, 'epoch': 5.55}\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 6475/23260 [19:45:44<51:10:31, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 06:32:25\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2246, 'grad_norm': 0.9389503002166748, 'learning_rate': 0.00016036113499570077, 'epoch': 5.56749785038693}\u001b[0m\n",
      "{'loss': 0.2246, 'grad_norm': 0.9389503002166748, 'learning_rate': 0.00016036113499570077, 'epoch': 5.57}\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 6500/23260 [19:50:18<51:09:30, 10.99s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 06:36:59\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2271, 'grad_norm': 0.9622130990028381, 'learning_rate': 0.00016012228909907328, 'epoch': 5.588993981083405}\u001b[0m\n",
      "{'loss': 0.2271, 'grad_norm': 0.9622130990028381, 'learning_rate': 0.00016012228909907328, 'epoch': 5.59}\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 6525/23260 [19:54:53<51:03:11, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 06:41:34\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2255, 'grad_norm': 0.7983298301696777, 'learning_rate': 0.00015988344320244579, 'epoch': 5.61049011177988}\u001b[0m\n",
      "{'loss': 0.2255, 'grad_norm': 0.7983298301696777, 'learning_rate': 0.00015988344320244579, 'epoch': 5.61}\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 6550/23260 [19:59:27<50:58:17, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 06:46:08\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2254, 'grad_norm': 0.793332576751709, 'learning_rate': 0.0001596445973058183, 'epoch': 5.631986242476354}\u001b[0m\n",
      "{'loss': 0.2254, 'grad_norm': 0.793332576751709, 'learning_rate': 0.0001596445973058183, 'epoch': 5.63}\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 6575/23260 [20:04:02<50:53:59, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 06:50:43\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2319, 'grad_norm': 0.7471923232078552, 'learning_rate': 0.0001594057514091908, 'epoch': 5.653482373172829}\u001b[0m\n",
      "{'loss': 0.2319, 'grad_norm': 0.7471923232078552, 'learning_rate': 0.0001594057514091908, 'epoch': 5.65}\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 6600/23260 [20:08:36<50:48:00, 10.98s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 06:55:17\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2374, 'grad_norm': 0.9336933493614197, 'learning_rate': 0.00015916690551256332, 'epoch': 5.674978503869303}\u001b[0m\n",
      "{'loss': 0.2374, 'grad_norm': 0.9336933493614197, 'learning_rate': 0.00015916690551256332, 'epoch': 5.67}\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 6606/23260 [20:09:42<50:47:20, 10.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 8900/23260 [27:09:23<43:45:43, 10.97s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2024-05-03 13:56:04\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.0929, 'grad_norm': 0.8553847074508667, 'learning_rate': 0.00013719308302283367, 'epoch': 7.6526225279449696}\u001b[0m\n",
      "{'loss': 0.0929, 'grad_norm': 0.8553847074508667, 'learning_rate': 0.00013719308302283367, 'epoch': 7.65}\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                   | 8907/23260 [27:10:40<43:43:03, 10.97s/it]"
     ]
    }
   ],
   "source": [
    "!autotrain llm --train \\\n",
    "    --project-name \"Llama2-KorQuAD2-dev-finetuning\" \\\n",
    "    --model \"hyunseoki/ko-ref-llama2-7b\" \\\n",
    "    --data-path \"seoma/korquad2-dev\" \\\n",
    "    --text-column \"text\" \\\n",
    "    --peft \\\n",
    "    --quantization \"int4\" \\\n",
    "    --lr 2e-4 \\\n",
    "    --batch-size 8 \\\n",
    "    --epochs 20 \\\n",
    "    --trainer sft \\\n",
    "    --model_max_length 256 \\\n",
    "    --save_total_limit 5 \\\n",
    "    --push-to-hub \\\n",
    "    --token \"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\" \\\n",
    "    --username \"seoma\" \\\n",
    "    --log wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a63c44-3c26-4398-9bc6-fdd761d6077d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
